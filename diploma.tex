\documentclass[bachelor, och, diploma, times]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа

				%    pract2      - отчет по практике,  2 курс
				%    pract3      - отчет по практике,  3 курс
				%    pract4      - отчет по практике,  4 курс
%    nir      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage{listings}
\usepackage{color}
\usepackage[english,russian]{babel}

% выделение ссылок цветом. чтобы включить- true
\usepackage[colorlinks=false]{hyperref}

\renewcommand{\rmdefault}{cmr} % курсив и полужирный включаются здесь.
\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}


\lstdefinestyle{mystyle}{
    numberstyle=\tiny,
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

% Кафедра (в родительном падеже)
\chair{теории функций и стохастического анализа} 
%\chairr{кафедра теории функций и стохастического анализа} % для практик!!!!
\chairr{завод "Тантал"}% для практик!!!!

% Тема работы

\title{Использование регуляризации для решения проблемы переобучения}

% Курс
\course{4}

% Группа
\group{412}

\department{механико-математического факультета}

% Специальность/направление код - наименование
\napravlenie{01.03.02 "--- Прикладная математика и информатика}
%\napravlenie{38.03.05 "--- Бизнес-информатика}
%\napravlenie{09.04.03 "--- Прикладная информатика}

% Для СТУДЕНТКИ!!!. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Георгиева Ивана Владимировича}

% Заведующий кафедрой
\chtitle{д.\,ф.-м.\,н., доцент} % степень, звание
\chname{С.\,П.\,Сидоров}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{д.\,ф.-м.\,н., доцент} %должность, степень, звание
\saname{С.\,П.\,Сидоров} % для 451 гр
%\saname{А.\,К.\,Смирнов} % для 412 гр

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
\patitle{ведущий программист}
\paname{Д.\,Э.\,Кнутов}

% Семестр (только для практики, для остальных
% типов работ не используется)
\term{5}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{производственная(базовая)}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{4}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{29.06.2019}
\practFinish{26.07.2019}

% Год выполнения отчета
\date{2021}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations
%\begin{description}
%    \item $|A|$  "--- количество элементов в конечном множестве $A$;
 %   \item $\det B$  "--- определитель матрицы $B$;
 %   \item ИНС "--- Искусственная нейронная сеть;
 %   \item FANN "--- Feedforward Artifitial Neural Network
%\end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro

В современном мире машинное обучение приобретает популярность при решении обширного класса задач. Практически в каждой из таких задач возникает проблема переобучения. При переобучении построенная для решения поставленной задачи модель может давать ответы близкие к реальным, на данных, используемых для обучения. Но при этом на остальных данных результаты могут значительно отличаться от ожидаемых.

Для решения этой проблемы используется много различных методов. Одним из наиболее актуальных является регуляризация. Так, в популярных библиотеках, предназначенных для машинного обучения, некоторые модели используют регуляризацию автоматически, при условии, что значение соответствующего параметра не предполагает обратного. Переобучение часто наступает при слишком больших значениях некоторых коэффициентов модели. Регуляризация не позволяет модели чересчур увеличивать коэффициенты, то есть снижает уровень переобучения.


Целью бакалаврской работы является разработка программного кода для решения задачи построения регрессии и методов машинного обучения с регуляризацией.

Для достижения поставленных целей в работе необходимо решить следующие задачи:

\begin{itemize}
\item изучить основные понятия машинного обучения;
\item показать случаи переобучения на примерах;
\item изучить основные виды регуляризации;
\item изучить язык программирования Python;
\item собрать данные для обучения модели из открытых источников;
\item разработать программу с использованием библиотек sklearn, csv, numpy;
\item обучить модель с использованием и без использования регуляризации;
\item провести тестирование программного кода;
\item провести сравнение результатов обучения.
\end{itemize}

Структура и содержание бакалаврской работы. Работа состоит из введения, пяти разделов, заключения, списка использованных источников, содержащего 20 наименований и двух приложений. Общий объём работы составляет 44 страницы.

В первом разделе рассматриваются основные понятия машинного обучения:

\begin{itemize}
\item общая постановка задачи машинного обучения,
\item классы задач машинного обучения,
\item классические задачи машинного обучения,
\item переобучение,
\item основные методы решения проблемы переобучения.
\end{itemize}

Второй раздел посвящён гребневой регрессии. 

\begin{itemize}
\item Даётся определение гребневой регрессии.
\item Приведён алгоритм градиентного спуска для гребневой регрессии.
\item Объясняется реализация алгоритма градиентного спуска для построения гребневой регрессии.
\end{itemize}

В третьем разделе изучаются виды и источники ошибок, возникающих при применении алгоритмов регрессии

В четвёртом разделе описывается регрессия по методу «лассо».

\begin{itemize}
\item Приведены отличия от гребневой регрессии.
\item Приведён алгоритм координатного спуска для регрессии методом наименьших квадратов.
\item Рассмотрен модифицированный алгоритм для регрессии лассо.
\item Разобрана реализация алгоритма координатного спуска для построения регрессии по методу «лассо».
\end{itemize}


В пятом разделе демонстрируются результаты программной реализации построения регрессий на языке Python.


\section{Основные понятия машинного обучения}

Приведём общую постановку задачи машинного обучения. Имеется множество объектов и множество ответов. Между ними существует некоторая неизвестная зависимость. Известно лишь конечное число пар "объект-ответ" \ составляющее обучающую выборку. По имеющимся данным следует построить алгоритм, который достаточно точно отображает неизвестную зависимость. То есть, способный для любых данных выдать ответ близкий к реальному.

Классы задач машинного обучения \cite{S12}:
\begin{itemize}
\item Обучение с учителем -- восстановление зависимости по известным примерам и ответам.
\item Обучение без учителя -- известно лишь множество объектов. Множество ответов отсутствует. Требуется, например, найти закономерности.
\end{itemize}

Классические задачи машинного обучения:
\begin{itemize}
\item Задача классификации. Множество объектов разделено некоторым образом на классы. Имеется обучающая выборка, содержащая пары "объект, класс". Требуется для произвольного объекта определить, к какому классу он мог бы принадлежать  \cite{B6}.
\item Задачи кластеризации. Предназначены как для разработки типологии, так и для проверки гипотез на основе исследования данных.
\item Задача регрессии. И множество объектов, и множество ответов являются численными данными. Требуется по конечному числу имеющихся точек восстановить исходную зависимость.
\end{itemize}

Функция прогноза для регрессии:

\begin{equation*}
		y_{pred} = w[0] x[0] + w[1] x[1] + … + w[p] x[p] + b.
\end{equation*}
где $x$ -- вектор признаков длины $p$ одной точки, $w$ и $b$ -- параметры модели, которые находятся её обучением и $y_{pred}$ -- предсказание. Для одного признака $y_{pred}$ -- линия, для двух -- плоскость, для большего числа -- гиперплоскость.

Существует множество различных линейных моделей регрессии. Разница между ними в том, как параметры модели узнаются из обучающих данных, и том, как можно контролировать сложность модели.

Линейная регрессия (метод наименьших квадратов) -- простейший линейный метод \cite{S11}. Модель находит параметры, которые минимизируют среднеквадратичную ошибку между прогнозом и истинным значением. Среднеквадратичная ошибка -- это сумма квадратов разностей между прогнозом и истинным значением. Если результаты на тренировочном и тестовом наборе очень близки, это означает, что происходит недообучение.

Обычно при глубоком обучении требуется минимизировать по $w$ следующую функцию потерь:

\begin{equation*}
	Q(w, X) = \frac{1}{l} ||X w - y||^2,
\end{equation*}

для этой задачи известно точное решение:

\begin{equation*}
	w^* = (X^T X)^{-1} X^T y.
\end{equation*}

Однако, при его использовании на практике из-за нескольких умножений и взятия обратной матрицы могут быть значительные потери точности. Кроме того, требуется обращать матрицу $X^T X$ размера $n \times n$, что требует $n^3$ действий, где $n$ -- число параметров, может быть достаточно большим  \cite{B7}.

В задаче линейной классификации в качестве функции потерь зачастую используют долю неправильных ответов:

\begin{equation*}
	Q(a, X) = \frac{1}{l} \sum_{i = 1}^{l} [a(x_i) \neq y_i].
\end{equation*}

У данной функции потерь есть несколько важных свойств:

\begin{itemize}
\item Она разрывная.
\item Она не гладкая.
\end{itemize}

Отсюда следует, что при её исследовании нельзя использовать методы гладкой оптимизации. Чтобы решить эту проблему, часто вместо $Q(a, X)$ рассматривают некую её гладкую верхнюю оценку, при минимизации которой будет минимизироваться и сама функция потерь  \cite{B2}.

\begin{equation*}
	Q(a, X) = \frac{1}{l} \sum_{i = 1}^{l} [a(x_i) \neq y_i].
\end{equation*}

Одна из основных проблем машинного обучения -- переобучение \cite{S14}. Это явление, при котором для элементов обучающей выборки модель показывает результат, близкий к корректному, а для любого другого работает гораздо хуже.
Это связано с тем, что при обучении модель обнаруживает в выборке случайные закономерности и в итоге "запоминает" \ все ответы.

Так как обучающая выборка конечна и неполна, а так же достоверно отличить случайные флуктуации от закономерностей невозможно, переобучение будет присутствовать практически всегда. 

Один из факторов, способствующих переобучению, -- чрезмерная сложность модели. Зачастую большое количество параметров поощряет подгонку под обучающее множество.

Методы борьбы с переобучением \cite{S8}:
\begin{itemize}
\item Перекрестная проверка -- данные разбиваются на $k$ частей. Обучение модели проходит на $k - 1$, тестирование на одной.
\item Ранняя остановка -- как только значение ошибки на тестовых данных начало превышать значение на обучающей выборке, прекращаем обучение.
\item Увеличение количества обучающих данных -- либо искусственно, специальным образом обрабатывая данные, либо путём сбора дополнительной информации.
\item Ансамбли моделей -- использование нескольких однотипных моделей параллельным или последовательным образом.
\item Регуляризация.
\end{itemize}

Регуляризация -- добавление дополнительных условий к задаче с целью предотвратить переобучение \cite{S19}.

В данной работе рассматривается применение $l_1$, $l_2$ - регуляризаций.

В этих методах используется линейная регрессия, но на этот раз модель добавит дополнительное ограничение на коэффициент $w$. Было установлено, что при достаточно больших значениях $||w||$ переобучение наступает чаще. Желательно, чтобы величина коэффициентов была как можно меньше, все значения $w$ должны быть близки к нулю.

В случае $l_1$ регуляризации требуется минимизировать

\begin{equation*}
	Q(w, X) + \lambda_1 ||w||.
\end{equation*}

В случае $l_2$ регуляризации:

\begin{equation*}
	Q(w, X) + \lambda_2 ||w||^2.
\end{equation*}

Иногда $l_1$ и $l_2$ регуляризации комбинируют. И минимизируется следующая функция потерь:

\begin{equation*}
	Q(w, X) + \lambda_1 ||w|| + \lambda_2 ||w||^2.
\end{equation*}

Такая модель называется $ELASTICNET$.


Кроме того существует техника регуляризации исключением.

В отличии от вышесказанных техник, регуляризация исключением не может применяться к линейным моделям и вместо изменения функции ошибки, будет меняться сама сеть.

Во время обучения нейросети часть её нейронов, за исключением входных и выходных, случайным образом временно удаляются. В результате чего получается изменённая сеть с меньшим числом нейронов. Далее берётся небольшое число примеров, на них происходит обучение сети. Обновляются соответствующие веса и смещения  \cite{B4}. Затем восстанавливаются удалённые нейроны и случайным образом выбирается новая группа для удаления.
Одна из возможных реализаций этого алгоритма -- вместо удаления фиксированного числа нейронов каждый раз, для каждого нейрона задать вероятность с которым он будет удаляться на новом шаге.

Получаем, когда удаляются разные нейроны, процесс обучения становится похож на обучение сразу нескольких различных нейросетей. И процедура исключения имеет схожий эффект с усреднением по большому числу нейросетей. Разные сети будут переобучаться по-разному и их общий результат может иметь более низкий уровень переобучения.

\section{Гребневая регрессия}

Известно, что использование многочленов более высокой степени в уравнении регрессии приводит к переобучению. Переобучение происходит, когда модель слишком хорошо "подходит" \  для обучающих данных и не обобщается на тестовые данные.

Обратимся к рисункам  \ref{fig:4.5} и \ref{fig:4.6}: переобучение также может произойти, если в уравнении регрессии слишком много независимых переменных или, если наблюдений слишком мало \cite{S16}.
Переобучение также связано с очень большими оценочными параметрами (весами) $\hat{w}$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig_4_5.png}
    \caption{\label{fig:4.5}
    Большое число наблюдений}
\end{figure}


\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{fig_4_6.png}
    \caption{\label{fig:4.6}
    Малое число наблюдений}
\end{figure}

Поэтому необходимо найти баланс между
\begin{itemize}
	\item мерой соответствия (насколько хорошо наша модель соответствует данным);
	\item величиной коэффициентов.
\end{itemize}

Таким образом, общая оценка модели представляет собой комбинацию меры соответствия и величиной коэффициентов. Мера соответствия представлена суммой квадратов разностей (RSS): небольшая указывает на хорошее соответствие. Мера величины коэффициентов -- это сумма абсолютных значений коэффициентов ($l_1$-норма) или сумма квадратов значений коэффициентов ($l_2$-норма). Они представлены следующим образом:


\begin{equation*}
	\left\|w_0\right\| + \left\|w_1\right\| + \cdots + \left\|w_n\right\| = \sum_{j = 0}^n \left\|w_j\right\| = \left\|w\right\|_1 \  (l_1 \  \text{норма}),
\end{equation*}

\begin{equation*}
	w_0^2  + w_1^2  + \cdots + w_n^2  = \sum_{j = 0}^n w_j^2  = \left\|w\right\|_2^2 \  (l_2 \  \text{норма}).
\end{equation*}

В гребневой регрессии считаем меру величины коэффициентов как $l_2$-норму. Таким образом, общая оценка

\begin{equation}
\label{eq:4.7.1}
	Total\text{ }Cost = RSS(w) + \left\|w\right\|_2^2.
\end{equation}


Цель при построении гребневой регрессии состоит в том, чтобы найти $\hat{w}$, чтобы минимизировать общие затраты в уравнении \eqref{eq:4.7.1}. Баланс между мерой соответствия и величиной коэффициентов достигается путем введения параметра настройки $\lambda$ следующим образом:

\begin{equation}
\label{eq:4.7.2}
	Total\text{ }Cost = RSS(w) + \lambda\left\|w\right\|_2^2.
\end{equation}

Если $\lambda = 0$, то задача суммы квадратов отклонений сводится к минимизации $RSS(w)$, то есть $\hat{w}^{LS}$.

Если $\lambda = \infty$, то общая стоимость равна $\infty$, когда ($\hat{w} \neq 0$) и общая стоимость равна $0$, когда ($\hat{w} = 0$).

Если $\lambda$ между, то $\left\|\hat{w}\right\|_2^2 \leq \left\|\hat{w}^{(LS)}\right\|_2^2$.

\subsection{Вычисление градиента гребневой регрессии}
\textbf{Закрытая форма решения}


Используя метод наименьших квадратов, получаем:

\begin{equation*}
	RSS(w)= (y - \textbf{H}w)^T (y - \textbf{H}w).
\end{equation*}

Общая оценка в случае гребневой регрессии равна

\begin{equation}
\label{eq:4.7.3}
\begin{gathered}
	Total\text{ }Cost = (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda\left\|w\right\|_2^2 = \\= (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda w^T w.
\end{gathered}
\end{equation}

Градиент уравнения \eqref{eq:4.7.3}

\begin{equation}
\label{eq:4.7.4}
\begin{gathered}
	\Delta\left[RSS(w) + \lambda\left\|w\right\|_2^2\right] = \Delta (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda\left\|w\right\|_2^2, \\
	\Delta Cost(w) = -2\textbf{H}^t (y - \textbf{H}w) + \lambda (2w) = \\
	= -2\textbf{H}^t (y - \textbf{H}w) + 2 \lambda Iw.
\end{gathered}
\end{equation}

Приравнивая уравнение \eqref{eq:4.7.4} к 0, получаем

\begin{equation}
\label{eq:4.7.5}
\begin{gathered}
	\Delta Cost(w) = 0,\\
	-2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda I w = 0,\\
	-\textbf{H}^T y + \textbf{H}^T \textbf{H} \hat{w} + \lambda I \hat{w} = 0,\\
	(\textbf{H}^T \textbf{H} + \lambda I) \hat{w} = \textbf{H}^T y,\\
	\hat{w}_{ridge} = (\textbf{H}^T \textbf{H} + \lambda I)^{-1} \textbf{H}^T y.
\end{gathered}
\end{equation}

Если $\lambda = 0$, то $(\hat{w}_{ridge} = \textbf{H}^T \textbf{H})^{-1} \textbf{H}^T y = \hat{w}^{LS}$.

Если $\lambda = 0$, то $(\hat{w}_{ridge} = 0)$.

Тот факт, что в уравнении \eqref{eq:4.7.3} добавлено слагаемое, оценивающее сложность модели, называется регуляризацией \cite{B3}. Сложность модели настраивается параметром $\lambda$.

\textbf{Градиентный спуск}
Поэлементный алгоритм градиентного спуска гребневой регрессии можно записать следующим образом:

\begin{equation}
\label{eq:4.7.6}
\begin{gathered}
	w_j^{(t + 1)} \leftarrow w_j^{(t)} + \Delta Cost(w),\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta (2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda w),\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta \left[-2 \sum_{i = 1}^n h_j(x_i)(y_i - \hat{y}_i(w^{(t)})) + 2 \lambda w_j^{(t)}\right],\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)}(1 - 2 \eta \lambda) + 2 \eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i(w^{(t)})).
\end{gathered}
\end{equation}

В уравнении \eqref{eq:4.7.6} $\eta$ -- размер шага и выражение $(1 - 2 \eta\lambda)$ всегда меньше или равно 1 пока $\eta > 0$, $\lambda > 0$. Выражение $2\eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i(w^{(t)}))$ -- коэффициент обновления из RSS.

Таким образом, $w_j^{(t)}$ уменьшается в $(1 - 2 \eta\lambda)$ на промежуточном этапе, а затем $w_j^{(t + 1)}$ увеличивается в коэффициент обновления $2\eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i (w^{(t)}))$.

Итак,
\begin{itemize}
	\item Для регрессии, которая строится методом наименьших квадратов, будет $w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta^*$ (коэффициент обновления);
	\item Для гребневой регрессии будет $w_j^{(t + 1)} \leftarrow (1 - 2\eta\lambda)w_j^{(t)} - \eta^*$ (коэффициент обновления).
\end{itemize}

Важным вопросом является выбор параметра настройки $\lambda$. Чтобы найти $\lambda$, можно использовать $k$-кратную перекрестную проверку.

Процесс включает подгонку $\hat{w}_{\lambda}$ к обучающему набору данных, тестирование производительности модели с $\hat{w}_{\lambda}$ на тестовом наборе для выбора $\lambda^*$ и, наконец, оценку ошибки обобщения модели с $\hat{w}_{\lambda^*}$. Средняя ошибка вычисляется следующим образом:

\begin{equation}
\label{eq:4.7.7}
\begin{gathered}
		CV(\lambda) = \frac{1}{k} \sum_{k = 1}^k error_k (\lambda).
\end{gathered}
\end{equation}



\subsection{Реализация алгоритма градиентного спуска для \\ построения гребневой регрессии}

В данном подразделе приводится описание приложения, основанного на книге \cite{Machine_Learning_with_R}, для построения гребневой регрессии, в котором используется набор данных $kc\_house\_data$. Набор данных содержит сведения о домах, проданных в период с мая 2014 года по май 2015 года в округе Кинг, штат Вашингтон, США. Эти данные охватывают также Сиэтл. Состоит из 21 переменной и 21613 наблюдений
Воспользуемся функцией "lm" \ в R и найдём коэффициенты регрессии. Зависимые переменные --  $sqft\_living$ и $bedrooms$, независимая -- $price$.

\lstinputlisting[language=R]{code_1.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms \\
97050.0942 & 305.2103 & -57429.9302 \\
\end{tabular}
\end{center}

Первая функция в приложении градиентного спуска гребневой регрессии -- это создание матрицы данных.

\lstinputlisting[language=R]{code_2.txt}

Функция predict\_output предсказывает целевые значения.

\lstinputlisting[language=R]{code_3.txt}

Согласно источнику \cite{S20} штраф $l_2$ получил свое название, потому что он заставляет веса иметь меньшую $l_2$ норму, чем в противном случае.
Следующая функция принимает начальные веса и матрицу признаков и предсказывает целевые значения с помощью функции predict\_output.
errors находятся по разнице в предсказанных и фактических значениях.
Значение gradient вычисляется с использованием матрицы признаков и errors $(\Delta cost(w) = 2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda w)$.
Веса обновляются путем вычитания произведения gradient и step\_size (см. уравнение \eqref{eq:4.7.6}).
Норма градиентов рассчитывается, чтобы проверить, меньше ли она tolerance в цикле «while».

\lstinputlisting[language=R]{code_4.txt}

Присвоим два разных значения параметру настройки $\lambda = 0$ и $\lambda = \infty$ и исследуем, как параметры (веса) корректируются. Рассмотрим модель с двумя функциями, то есть sqft\_living и bedrooms.

\lstinputlisting[language=R]{code_5.txt}

\begin{center}
\begin{tabular}{c}
[1] "Gradient descent converged at iteration: 3002" \\
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_6.txt}

\begin{center}
\begin{tabular}{c}
$\left[,1\right]$\\
$\left[1,\right] -5.560047e-17$\\
$\left[2,\right] 7.885384e-01$\\
$\left[3,\right] -1.489809e-01$\\
\end{tabular}
\end{center}

Коэффициенты с $\lambda = 0$ такие же, как для OLS регрессии. Сравним полученные результаты с выводом программы при использовании функции «lm» в R:

\lstinputlisting[language=R]{code_7.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms \\
2.600634e-17 & 7.885384e-01 & -1.489809e-01 \\
\end{tabular}
\end{center}

Результаты совпадают.
R имеет функцию «glmnet» в пакете glmnet. Характеристики и результат принадлежат классу matrix. Аргумент «alpha» равен 0 для гребневой регрессии гребня и 1 для регрессии лассо. Допуск, определенный в алгоритме, -- это «порог», который по умолчанию равен 1e-07. Есть много других параметров, которые можно изучить, набрав help(glmnet).
Проверим результаты с помощью функции «glmnet» в R (таблица \ref{table:4.7}).

\lstinputlisting[language=R]{code_8.txt}

\begin{table}
\caption{\label{table:4.7}Сравнение коэффициентов гребневой регрессии с $\lambda = 0$ для приложения, приведённого в данной работе, модели регрессии OLS и glmnet}
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
             &  Application &  OLS     &  glmnet \\
\hline
(Intercept)  &  0.00000     &  0.00000 &  0.00000\\
\hline
sqft\_living &  0.78854     &  0.78854 &  0.78853\\
\hline
bedrooms     & -0.14898     & -0.14898 & -0.14898\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{\label{table:4.8}Сравнение коэффициентов гребневой регрессии с $\lambda = 100000$ для приложения, приведённого в данной работе, и glmnet}
\begin{center}
\begin{tabular}{c|c|c}
\hline
             & Application& glmnet\\
\hline                     
(Intercept)  & 0.00000    & 0e+00 \\
\hline                     
sqft\_living & 0.06679    & 1e-05 \\
\hline                     
bedrooms     & 0.02696    & 0e+00 \\
\hline
\end{tabular}
\end{center}
\end{table}

\lstinputlisting[language=R]{code_9.txt}

Реализованное приложение показывает достаточно высокий уровень точности.
Далее используем $\lambda = 100000$

\lstinputlisting[language=R]{code_10.txt}

Эта модель имеет низкую предвзятость \cite{S10}.
Выполним проверку результатов с помощью функции glmnet в R (таблица \ref{table:4.8}).

\lstinputlisting[language=R]{code_11.txt}

Действительно, получаем одинаковые результаты, т.е. все коэффициенты очень близки к $0$.
Покажем графики уравнений гребневой регрессии для двух значений $\lambda$ на рисунке $\ref{fig:4.8}$. На оси абсцисс - нормированные значения sqft\_living. На оси ординат - нормированные значения price. Модель гребневой регрессии с градиентным спуском переходит в нулевую модель с отсечением 0 (высокое смещение), когда $\lambda = \infty$, и модель регрессии LS, когда $\lambda = 0$.
Найдём оптимальное значение $\lambda$с помощью перекрестной проверки.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=345pt]{fig_4_8.png}
    \caption{\label{fig:4.8}
    Гребневая регрессия с $\lambda = 0$ и $\lambda = 100000$}
\end{figure}

\section{Оценка производительности}

Ранее было упомянуто об оценке модели. Оценка модели -- это мера ошибок модели, то есть того, насколько точно модель может предсказать фактическое значение. То есть, если модель не допускает ошибок прогнозирования, стоимость, приписываемая модели, равна нулю, и, следовательно, ее мера точности составляет $100 \% $.
Согласно \cite{S9} стоимость регрессионной модели измеряется функцией потерь, определяемой как

\begin{equation}
\label{eq:4.8.1}
\begin{gathered}
	Loss(y, f_{\hat{w}}(x)) = (y - f_{\hat{w}}(x))^2 \text{ -- ошибка в квадрате},\\
	Loss(y, f_{\hat{w}}(x)) = \left|y - f_{\hat{w}}(x)\right| \text{ -- модуль ошибки}.
\end{gathered}
\end{equation}

О мере точности модели можно судить по оценке модели на обучающих и тестовых данных. При обучении модели модель пытается соответствовать обучающим данным. Следовательно, с увеличением сложности модели ошибка обучения уменьшается. Очень низкая ошибка обучения не является хорошим показателем точности модели, если обучающие данные не включают «вселенную», то есть все возможные данные, соответствующие данной задаче.
Следовательно, эффективность модели можно оценить только по степени ее точности с тестовыми данными.
\textbf{Ошибка обобщения} -- это ошибка модели на данных, не принимавших участие в процессе обучения. 
Обсуждаемые здесь ошибки модели зависят от сложности и других алгоритмических характеристик модели. Но в самих данных могут быть ошибки. В следующем подразделе приведены возможные способы решения данной проблемы.

\subsection{Источники ошибок}

Ошибки могут возникать из данных обучения модели и из тестовых данных. Однако существуют также три других источника ошибок в модели -- шум, смещение и дисперсия.
\textbf{Шум} всегда проникает в данные, потому что практически невозможно записать без потери информации, например, «человеческие эмоции», «проблемы во взаимоотношениях», «глобальные проблемы» и так далее.
На основании имеющихся данных в модели также проникают \textbf{смещения} и \textbf{отклонения}. Если обучить модель на основе обучающих данных, в которых нет шума (noise), и оценить ту же модель на тестовых данных, которые содержат шум, получим модель с очень низкой точностью.
Следовательно ошибка прогнозирования в модели состоит из трех компонентов: шума, смещения и дисперсии, которые можно записать как

\begin{equation}
\label{eq:4.8.2}
\begin{gathered}
	Prediction \ Error = \varsigma^2 + MSE\left[f_{\hat{w}}(x)\right]\\
	= \varsigma^2 + \left[bias(f_{\hat{w}}(x))\right]^2 + var(f_{\hat{w}}(x)).
\end{gathered}
\end{equation}

Подходящая модель выбирается путем выбора параметра настройки $\lambda$, который управляет сложностью модели. Это делается с помощью перекрестной проверки. Выбрав подходящую модель, оцениваем ошибку обобщения.
Рассмотрим house\_train\_data и house\_test\_data, которые были разделены
из набора данных kc\_house. Наборы данных для обучения и тестирования имеют 10 807 и 10 806
наблюдения соответственно.

\lstinputlisting[language=R]{code_12.txt}

Обучим модель гребневой регрессии с $\lambda = 0$, используя обучающие данные, то есть реплицируем OLS регрессию и используем модель для прогнозирования тестовых данных с $\lambda = 10$.

\lstinputlisting[language=R]{code_13.txt}

\begin{center}
\begin{tabular}{c}
[1] "MSE\_lambda\_10 = 0.903155"
\end{tabular}
\end{center}

Среднеквадратичная ошибка для этой модели составляет $0.903155$.
R имеет встроенную функцию перекрестной проверки cv.glmnet(). По умолчанию функция выполняет 10-кратную перекрестную проверку, но это можно изменить с помощью аргумента «nfolds».
Выполняем 10-кратную перекрестную проверку данных обучения и находим значение $\lambda$, для которого ошибка перекрестной проверки наименьшая.

\lstinputlisting[language=R]{code_14.txt}

\begin{center}
\begin{tabular}{c}
[1] "ideal\_lambda = 0.076901"
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_15.txt}

\begin{center}
\begin{tabular}{c}
[1] "MSE with ideal\_lambda = 0.495869"
\end{tabular}
\end{center}

Среднеквадратичная ошибка при $\lambda = 0.076901$ составляет $0.495869$, то есть MSE упала на $45\%$.
Выясним коэффициенты этой модели

\lstinputlisting[language=R]{code_16.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept)   & sqft\_living & bedrooms\\
-1.397232e-16 & 6.998124e-01 & -8.978171e-02
\end{tabular}
\end{center}

Визуализируем рассмотренное выше уравнение гребневой регрессии. Ось абсцисс -- sqft\_living. Ось ординат -- price. На рисунке \ref{fig:4.9} представлен график уравнения регуляризованной гребневой регрессии.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=345pt]{fig_4_9.png}
    \caption{\label{fig:4.9}
    Гребневая регрессия с $\lambda = 0.076901$}
\end{figure}

Покажем, как меняются нормированные коэффициенты при разных значениях $\lambda$. Построим график: на оси абсцисс -- $\lambda$, на оси ординат -- значения соответствующих коэффициентов. На рисунке \ref{fig:4.10} видно, что коэффициенты меняются постепенно от $\lambda = 0$ до $\lambda \rightarrow \infty$. Резкого уменьшения до нуля не происходит.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=345pt]{fig_4_10.png}
    \caption{\label{fig:4.10}
    Нормированные коэффициенты гребневой регрессии при изменении $\lambda$. Коэффициенты не падают резко до 0 при $\lambda \rightarrow \infty$}
\end{figure}

\subsection{Дилемма смещения -- дисперсии в гребневой регрессии}

Большое значение $\lambda$ приведет к высокому смещению (простая модель) и, следовательно, к низкой дисперсии $(\hat{w} = 0$, для $\lambda = \infty)$.
Небольшое значение $\lambda$ приведет к низкому смещению и, как следствие, большой дисперсии \cite{B5}.

\section{Регрессия по методу «лассо»}

Если имеется «широкий» набор функций (скажем, $1e+10$), гребневая регуляризация может создать вычислительные проблемы, поскольку выбраны все функции.
Алгоритм лассо отбрасывает менее важные или избыточные характеристики, переводя их коэффициенты в ноль. Это позволяет более качественно интерпретировать функции, а также сокращает время вычислений. Регрессия по методу «лассо» (регуляризованная регрессия $l_1$) использует норму $l_1$ в качестве штрафа,
вместо нормы $l_2$ в гребневой регрессии.
Перед тем, как перейти к целевой функции регрессии лассо, вернёмся к алгоритму гребневой регрессии . Гребневая регрессия выбирает параметры $\beta$ с минимальной RSS при условии, что норма $l_2$ параметров $\beta_1^2 + \beta_2^2 + \cdots + \beta_n^2 \leq t$.
На рисунке \ref{fig:4.11} показаны изолинии RSS и ограничения параметров для гребневой регрессии и регрессии лассо.
Контуры RSS представляют собой эллипсы, центрированные по методу наименьших квадратов (точка красного цвета). Ограничение построено для различных значений параметров $\beta$. В случае гребневой регрессии $\beta_1^2 + \beta_2^2 \leq t$, ограничение гребня. Ограничение принимает форму круга для двух параметров и становится сферой с большим количеством параметров. Первая точка соприкосновения контура RSS с окружностью -- это точка, описывающая параметры гребня $\beta_1$ и $\beta_2$. Значения $\beta$ в большинстве случаев оказываются ненулевыми. Если $t$ мало, параметры будут маленькими, а если велико, будет стремиться к решению по методу наименьших квадратов.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=345pt]{fig_4_11.png}
    \caption{\label{fig:4.11}
    Левый график показывает контуры RSS гребневой регрессии и ограничение $\beta_1^2 + \beta_2^2 \leq t$ (ограничение гребня). Правый график показывает RSS-контуры регрессии лассо и ограничение на коэффициенты лассо $\left|\beta_1\right| + \left|\beta_2\right| \leq t$ (ограничение лассо)}
\end{figure}

В случае регрессии лассо параметры $\beta$ выбираются такими, что $\left|\beta_1\right| + \left|\beta_2\right| \leq t$, ограничение лассо, для минимального RSS.
Как показано на правом графике рисунка \ref{fig:4.11} для регрессии лассо с двумя параметрами, ограничение имеет форму ромба с четырьмя углами; для более чем двух элементов контур становится ромбовидным с множеством углов. Если контур RSS касается угла, он заставляет одну из $\beta$ стать нулевой.
Можно переписать общую оценку в формуле \eqref{eq:4.7.3} для регрессии лассо как

\begin{equation}
\label{eq:4.9.1}
\begin{gathered}
	Total\text{ }Cost = RSS(w) + \lambda \left\|w\right\|_1.
\end{gathered}
\end{equation}

Если $\lambda = 0, то  \hat{w}^{lasso} = \hat{w}^{LS}$.
Если $\lambda = \infty, то \hat{w}^{lasso} = 0$.
Если $\lambda$ между, то $0 \leq \hat{w}^{lasso} \leq \hat{w}^{LS}$.

Лассо выбирает параметры $\beta$ с минимальным RSS, при условии, что $l_1$ норма параметров $(\left|\beta_1\right| + \left|\beta_2\right| + \cdots + \left|\beta_n\right|) \leq tolerance$.
Ранее было показано, что оптимальное решение задачи минимизации лассо находится в начале координат, и поэтому градиент вычислить нельзя.
Поэтому решением является выпуклый алгоритм оптимизации, называемый координатным спуском  \cite{S15}. Этот алгоритм пытается минимизировать

\begin{equation}
\label{eq:4.9.2}
\begin{gathered}
	f(w) = f(w_0, w_1, \cdots, w_n).\\
\end{gathered}
\end{equation}

Алгоритм координатного спуска можно описать следующим образом:

\begin{equation}
\label{eq:4.9.3}
\begin{gathered}
	\text{Initialize } \hat{w},\\
	\text{while not converged, pick a coordinate j},\\
	\hat{w}_j \leftarrow \underset{min}{f}(\hat{w}_0. \hat{w}_1, \cdots, w, \hat{w}_{j + 1}, \cdots, \hat{w}_n).
\end{gathered}
\end{equation}

Если выбирать следующую координату случайным образом, этот алгоритм станет алгоритмом стохастического координатного спуска.
При координатном спуске не нужно выбирать размер шага.

\subsection{Координатный спуск для регрессии наименьших \\ квадратов}

Ниже приведен алгоритм координатного спуска для регрессии наименьших квадратов, по одной координате за раз.

\begin{equation}
\label{eq:4.9.4}
\begin{gathered}
RSS(w) = \sum_{i = 1}^n (y_i - \sum_{j = 0}^n w_j h_j(x_i))^2.\\
\text{Зафиксируем все координаты $w_{-j}$ и возьмем частную производную по $w_j$:}\\
\frac{\partial}{\partial w_j} = -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{j = 0}^n w_j h_j(x_i))^2,\\
= -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{k \neq j}w_k h_k(x_i) - w_j h_j(x_i)),\\
= -2 \sum_{i = 1}^m h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i)) + 2 w_j \sum_{i = 1}^n h_j(x_i)^2,\\
\text{Пояснения:}\\
\text{по определению нормированных функций} \sum_{i = 1}^n h_j(x_i)^2 = 1,\\
\sum_{i = 1}^n h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i) = \rho_j 
= -2 \rho + 2 w_j.\\
\text{Приравняв частичные производные к 0, получим}\\
w_j = \rho_j.
\end{gathered}
\end{equation}

\subsection{Координатный спуск для регрессии лассо}

Ниже приведен псевдокод спуска координат для регрессии лассо, по одной координате за раз.


\begin{equation}
\label{eq:4.9.5}
\begin{gathered}
\text{Initialize } \hat{w},\\
\text{while not converged},\\
\text{for j in } 0,1,2, \cdots,n\\
\text{compute: } \rho_j = \sum_{i = 1}^n h_j(x_i)(y_i - \hat{y}_i(\hat{w}_j)),\\
\text{set: } w_j = \begin{cases}
\rho_j + \frac{\lambda}{2} \text{ if } \rho_j < - \frac{\lambda}{2},\\
0 \text{ if } \rho_j \text{ in } \left[-\frac{\lambda}{2}, \frac{\lambda}{2}\right],\\
\rho_j - \frac{\lambda}{2} \text{ if } \rho_j > \frac{\lambda}{2}.
\end{cases}\\
\end{gathered}
\end{equation}

\subsection{Написание приложения для координатного спуска \\ регрессии лассо}

На основе книги \cite{Machine_Learning_with_R} приведём алгоритм регрессии лассо.

Функция get\_features подбирает данные на основе выбранных функций.

\lstinputlisting[language=R]{code_17.txt}

Функция add\_constant\_term добавляет столбец единиц к матрице признаков.

\lstinputlisting[language=R]{code_18.txt}

Функция construct\_matrix принимает имена функций и выходные данные из data.frame. Затем он выбирает значения функций и выходные значения, вызывая функцию get\_features, и возвращает их в виде списка.

\lstinputlisting[language=R]{code_19.txt}

Функция normalize\_features вычисляет норму для каждой функции и нормализует feature\_matrix.

\lstinputlisting[language=R]{code_20.txt}

Функция predict\_output предсказывает выходные значения.

\lstinputlisting[language=R]{code_21.txt}

В уравнении \eqref{eq:4.9.4} было показано, что $\rho_j = \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{k \neq j} w_k h_k(x_i))$. Для понимания последующего кода, в котором вычисляется $\rho$, рассмотрим набор данных с тремя функциями, в котором требуется найти $\rho_2$. Это можно записать как


\begin{equation}
\label{eq:4.9.6}
\begin{gathered}
\rho_2 = h_2(x_2) \left[(y_1 + y_2 + y_3) - (\hat{w}_1 h_1(x_1) + \hat{w}_3 h_3(x_3))\right] = \\
= h_2(x_2) \left[(y_1 + y_2 + y_3) - (\hat{y}_1 + \hat{y}_3)\right] = \\
= h_2(x_2) \left[(y_1 - \hat{y}_1) + (y_2 - \hat{y}_2) + (y_3 - \hat{y}_3) + \hat{y}_2\right] = \\
= h_2(x_2) \left[(target - predicted) + \hat{w}_2 h_2(x_2)\right].\\
\end{gathered}
\end{equation}

\lstinputlisting[language=R]{code_22.txt}

Далее реализуем функцию координатного спуска, которая минимизирует функцию стоимости для одного признака $i$. Функция возвращает новый вес для признака $i$.

\lstinputlisting[language=R]{code_23.txt}

Итак, при наличии функции, которая оптимизирует функцию стоимости по одной координате, реализуем циклический спуск координат, где отсортируем координаты от 1 до $n$ по порядку и повторяем.

\lstinputlisting[language=R]{code_24.txt}

\subsection{Реализация координатного спуска}

При обучении будет использоваться набор данных KC house, состоящий из 21613 наблюдений. Разделим его на тренировочные и тестовые данные, состоящие из 19451 и 2162 наблюдений соответственно.
Будем искать параметры для «sqft\_living» и «bedrooms», и выполним функцию \\ lasso\_cyclical\_coordinate\_descent со штрафом $l_1 = 0$, что аналогично регрессии LS.

\lstinputlisting[language=R]{code_25.txt}

\begin{center}
\begin{tabular}{c}
[1] 97050.0942 305.2103 -57429.9302
\end{tabular}
\end{center}

Сравним результаты с функциями glm и lm в R.

\lstinputlisting[language=R]{code_26.txt}

\begin{center}
\begin{tabular}{c}
[1] 97052.1826 305.1896 -57417.7511
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_27.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms\\
97050.0942  & 305.2103    & -57429.9302
\end{tabular}
\end{center}

После выполнения алгоритма для регрессии лассо получаем схожие результаты.

В регрессии лассо, когда $\lambda = 0$, решение находится методом наименьших квадратов, а когда $\lambda$ становится достаточно большим, регрессия лассо дает нулевую модель, в которой все оценки коэффициентов равны нулю.
На рисунке \ref{fig:4.12} построены графики семи коэффициентов для регрессии лассо при изменении $\lambda$ от 0 до $1e+10$. Ось абсцисс -- натуральный логарифм $\lambda$. Ось ординат -- значения соответствующих параметров. Можно заметить, что между двумя крайностями подгонки LS и нулевой модели, в зависимости от значения $\lambda$, лассо создает модели с любым количеством переменных, и большинство из них отбрасывается по мере увеличения значения $\lambda$.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=345pt]{fig_4_12.png}
    \caption{\label{fig:4.12}
    Коэффициенты регрессии Лассо при изменении $\lambda$. Коэффициенты резко уменьшаются почти до 0 при $\lambda \rightarrow \infty$
    }
\end{figure}

\subsection{Компромисс отклонения в регрессии лассо}

Большое значение $\lambda$ приведет к высокому смещению (простая модель) и, следовательно, к низкой дисперсии ($\hat{w} = 0$, для $\lambda = \infty$).
Небольшое значение $\lambda$ приведет к низкому смещению и, как следствие, большой дисперсии.

\section{Разработка программной реализации построения регрессий с регуляризацией}

Выполним реализацию построения регрессий на языке программирования Python.

В алгоритмах используется SGDClassifier  \cite{S13}. SGDClassifier -- линейный классификатор из библиотеки sklearn. Реализует обучение с помощью стохастического градиентного спуска. Поддерживает $l_1$ и $l_2$ регуляризации.

\subsection{Постановка диагноза по симптомам}

Приведём описание алгоритма применения регуляризации при построении модели, показывающей диагнозы пациентов.

\begin{enumerate}
\item Импортируем нужные библиотеки.
\item Поиск в открытых источниках данных по диагнозам. В работе используются данные с платформы kaggle.
\item Предварительная подготовка данных -- чтение всех симптомов и диагнозов. Для каждого диагноза набор симптомов формируется следующим образом: создаётся словарь, где ключом является симптом, а значением -- 0 или 1 в зависимости от наличия симптома для соответствующего диагноза.
\item Обучение модели без регуляризации и двух моделей с использованием регуляризации с помощью SGDClassifier.
\item Сравнение полученных результатов.
\end{enumerate}

Используемые данные по диагнозам хранятся в двух файлах:

\begin{enumerate}
\item в файле $"Symptom-severity.csv"$ хранится список всевозможных симптомов, которые могут встречаться в основном наборе данных и численная оценка их серьёзности;
\item в файле $"dataset.csv"$ находится список диагнозов и соответствующих им симптомом.
\end{enumerate}

Теперь приведём подробное описание действий программы.

С помощью библиотеки $csv$ считываем из файла $"Symptom-severity.csv"$ список симптомов и сохраняем его в словарь $symptoms$

\lstinputlisting[language=Python]{codePython00.txt}


Определяем функцию, которая принимает на вход список симптомов $old\_list$, а возвращает вектор, в котором на местах, соответствующих симптомам из $old\_list$ стоят положительные числа, взятые из словаря $symptoms$, а на всех остальных стоят нули.

\lstinputlisting[language=Python]{codePython01.txt}



Из файла $"dataset.csv"$ построчно берутся диагнозы и их симптомы. Диагнозы помещаются в список $disease$, а список симптомов, преобразованный с помощью функции $to\_vec$ в $diseaseSymptoms$.

\lstinputlisting[language=Python]{codePython02.txt}



С помощью функции $train\_test\_split$ из библиотеки $sklearn$ разбиваем данные, полученные на предыдущем шаге на обучающую и тестовую выборки

\lstinputlisting[language=Python]{codePython03.txt}


С помощью $SGDClassifier$ обучаем 3 модели: $clf$ -- без регуляризации, $clf1$ -- $l_1$-регуляризации, $clf2$ -- $l_2$-регуляризации.

\lstinputlisting[language=Python]{codePython04.txt}

Для проверки вводим случайный тест и выводим результат применения к нему моделей. С помощью функции $accuracy\_score$ из библиотеки $sklearn$ считаем точность каждой модели на обучающей и тестовой выборке:

\lstinputlisting[language=Python]{codePython05.txt}

Результат выполнения:

\begin{Verbatim}[commandchars=\\\{\}]
['Diabetes']
['Diabetes']
['Diabetes']
1.0
0.9253421816673579
1.0
0.9464952301949399
1.0
0.9477395271671506
\end{Verbatim}
    
Все три модели показали идеальный результат на обучающей выборке. На тестовой результаты соответствуют ожиданиям: без применения регуляризации точность ниже, чем с её применением.

\subsection{Выявление предрасположенности к болезням сердца}

Краткий алгоритм работы программы:
\begin{enumerate}
\item Импортируем нужные библиотеки.
\item С платформы Kaggle получаем набор данных $Heart Disease Dataset.csv$  \cite{S17}.
\item С помощью библиотеки csv производим чтение данных. Каждая модель должна подобрать значения столбца с названием target, используя остальные столбцы.
\item Обучение модели без регуляризации и двух моделей с использованием регуляризации с помощью SGDClassifier.
\item Сравнение полученных результатов.
\end{enumerate}

В файле $Heart Disease Dataset.csv$ содержатся следующие столбцы:
\begin{enumerate}
\item age -- возраст человека в годах;
\item sex -- пол человека (1 -- если мужской, 0 -- женский);
\item cp -- тип боли в груди, приведённый к числовому формату;
\item trestbps -- артериальное давление в покое;
\item chol -- уровень холестерина в крови;
\item fbs -- уровень сахара в крови натощак;
\item restecg -- результаты электрокардиографии в покое;
\item thalach -- максимальный уровень пульса;
\item exang -- наличие стенокардии, вызванная физической нагрузкой;
\item oldpeak -- депрессия сегмента ST при нагрузке в сравнении с состоянием покоя;
\item slope -- наклон сегмента ST при пиковой нагрузке;
\item ca -- количество крупных сосудов, окрашенных флурозопией;
\item target -- наличие предрасположенности к болезням сердца.
\end{enumerate}
Всего в описанном наборе данных имеется 303 строки.

С помощью библиотеки csv из имеющегося набора данных считываем значения. В списке disease храним значения для столбца target. В списке diseaseSymptoms -- все остальные значения из соответствующей строки.
\lstinputlisting[language=Python]{codeS00.txt}

Разделяем данные на обучающие и тестовые:
\lstinputlisting[language=Python]{codeS01.txt}

Обучаем модель без использования регуляризаций и с применением $l_1$, $l_2$ регуляризаций.
\lstinputlisting[language=Python]{codeS02.txt}

Считаем точность полученных моделей на тестовых данных с помощью функции $accuracy\_score$.

\lstinputlisting[language=Python]{codeS03.txt}

Итак, на тестовом наборе данных модель без использования регуляризации -- $70.5\%$. С использованием $l_1$ регуляризации -- $75.4\%$, $l_2$ -- $77\%$.

Построим матрицу несоответствий для каждой модели  \cite{S18}. С помощью функции $confusion\_matrix$ посчитаем для каждой модели вероятности дать истинно положительный, ложно положительный, ложно отрицательный и истинно отрицательный ответ:

\lstinputlisting[language=Python]{codeS04.txt}

Оформим вывод в виде таблиц \ref{tableAns1}, \ref{tableAns2} и \ref{tableAns3}.

\begin{table}
\caption{Матрица для модели без использования регуляризации}
\label{tableAns1}
\begin{tabular}[!htb]{|l|l|}
\hline
29.5\%  & 14.75\% \\ \hline
14.75\% & 41\%    \\ \hline
\end{tabular}
\end{table}

\begin{table}
\caption{Матрица для модели с использованием $l_1$ регуляризации}
\label{tableAns2}
\begin{tabular}[!htb]{|l|l|}
\hline
27.9\% & 16.4\% \\ \hline
8.2\%  & 47.5\% \\ \hline
\end{tabular}
\end{table}

\begin{table}
\caption{Матрица для модели с использованием $l_2$ регуляризации}
\label{tableAns3}
\begin{tabular}[!htb]{|l|l|}
\hline
34.4\% & 9.9\%  \\ \hline
13.1\% & 42.6\% \\ \hline
\end{tabular}
\end{table}

Сумма диагональных значений для каждой модели равняется значениям, полученным с помощью функции accuracy\_score как сумма истинно положительных и истинно отрицательных ответов.
% Раздел "Заключение"
\conclusion
Поставленные задачи были успешно выполнены. Были рассмотрены основные понятия машинного обучения, осуществлено знакомство с гребневой регрессией, регрессией по методу "лассо". Успешно освоен язык программирования Python. Проведена разработка программы для решения задачи построения регрессии с регуляризацией. 
%Библиографический список, составленный вручную, без использования BibTeX
%
\begin{thebibliography}{99}
%1

\bibitem{S12} Машинное обучение — это легко [Электронный ресурс]: [сайт]. - URL: https://habr.com/ru/post/319288/  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.
   
\bibitem{B6} Рутковская Д., Пилиньский М., Рутковский Л. Нейронные сети,
генетические алгоритмы и нечеткие системы. / Пер. с польского И.Д. Рудинского - М.: Горячая линия - Телеком, -- 2006. - 452 с.

\bibitem{S11} Machine Learning. Linear Models. Part 1. [Электронный ресурс]: [сайт]. - URL: https://medium.com/pharos-production/machine-learning-linear-models-part-1-312757aab7bc  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.

\bibitem{B7} Головко В. А. Нейронные сети: обучение, организация и применение. / В. А. Головко, В. В. Краснопрошин. – Минск: БГУ, -- 2017. – 263 с.

\bibitem{B2} Галушкин А.И. Нейрокомпьютеры и их применение. Книга 3. Нейрокомпьютеры / А.И. Галушкин М.: ИПРЖР, -- 2000. - 528 с.

\bibitem{S14} Машинное обучение (курс лекций, К.В.Воронцов) [Электронный ресурс]: [сайт]. - URL: www.machinelearning.ru/wiki/index.php? title=Машинное\_обучение\_\%28курс\_лекций\%2C\_К.В.Воронцов\%29  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.

\bibitem{S8} Нейросети и глубокое обучение, глава 3, ч.2: почему регуляризация помогает уменьшать переобучение? [Электронный ресурс]: [сайт]. - URL: https://habr.com/ru/post/459816/ (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.

\bibitem{S19} Регуляризация - Линейные модели: классификация и практические аспекты | Coursera [Электронный ресурс]: [сайт]. - URL: https://ru.coursera.org/lecture/supervised-learning/rieghuliarizatsiia-sR94Q  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.

\bibitem{B4} Галушкин А.И. Нейрокомпьютеры и их применение. Книга 1. Теория нейронных сетей / А.И. Галушкин М.: ИПРЖР, -- 2000. - 416 с.
   
\bibitem{S16} L1 и L2-регуляризация для логистической регрессии [Электронный ресурс]: [сайт]. - URL: https://craftappmobile.com/l1-и-l2-регуляризация-для-логистической-р/  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.
      
\bibitem{B3} Осовский С. Нейронные сети для обработки информации / Пер. с польского И.Д. Рудинского М.:Финансы и статистика, -- 2002. - 344 с.
        
\bibitem{Machine_Learning_with_R} Ghatak, A. Machine Learning with R / A. Ghatak. - Singapore: Springer Nature, -- 2017. - 210 pp.
  
\bibitem{S20} The difference between L1 and L2 regularization [Электронный ресурс]: [сайт]. - URL: https://explained.ai/regularization/L1vsL2.html (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.
  
\bibitem{S10} How to Improve a Neural Network With Regularization [Электронный ресурс]: [сайт]. - URL: https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3 (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.

\bibitem{S9} Neural Networks and Deep Learning [Электронный ресурс]: [сайт]. - URL: http://neuralnetworksanddeeplearning.com/chap3.html (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.

\bibitem{B5} Горбань А.Н., Россиев Д.А. Нейронные сети на персональном компьютере. / А.Н. Горбань, Д.А. Россиев. - Новосибирск: Наука. Сибирская издательская фирма РАН, -- 1996. - 276 с.
   
\bibitem{S15} Простыми словами о методах решения проблем с переобучением [Электронный ресурс]: [сайт]. - URL: https://newtechaudit.ru/overfitting/  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.
   
\bibitem{S13} sklearn.linear\_model.SGDClassifier -- scikit-learn 0.24.2 documentation [Электронный ресурс]: [сайт]. - URL: https://scikit-learn.org/stable/modules/generated/sklearn.linear\_model.SGDClassifier.html  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.
   
\bibitem{S17} Heart Disease Dataset | KaggleHeart Disease Dataset | Kaggle [Электронный ресурс]: [сайт]. - URL: https://www.kaggle.com/zeeshanmulla/heart-disease-dataset  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.англ.
    
\bibitem{S18} Понимание метрик классификации Data Science в Scikit-Learn в Python [Электронный ресурс]: [сайт]. - URL: https://www.machinelearningmastery.ru/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019/  (дата обращения: 04.06.2021). - Загл. с экрана. - Яз.рус.
    
\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
%\bibliographystyle{ugost2008}
%\bibliographystyle{ugost2008_In} как в каталоге
\bibliographystyle{utf8gost780u} %фамилии курсивом
%\bibliographystyle{utf8gost71u} %фамилии рядом
%\bibliographystyle{ugost2003s}


%\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix

\section{Программный код для задачи постановки диагноза по симптомам}

\lstinputlisting[language=Python]{codePython.txt}

\pagebreak

\section{Программный код для задачи выявления предрасположенности к болезням сердца}

\lstinputlisting[language=Python]{codeS.txt}

\end{document}
