\documentclass[bachelor, och, pract4, times]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа

				%    pract2      - отчет по практике,  2 курс
				%    pract3      - отчет по практике,  3 курс
				%    pract4      - отчет по практике,  4 курс
%    nir      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage{listings}
\usepackage{color}
\usepackage[english,russian]{babel}

% выделение ссылок цветом. чтобы включить- true
\usepackage[colorlinks=false]{hyperref}

\renewcommand{\rmdefault}{cmr} % курсив и полужирный включаются здесь.
\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\lstdefinestyle{mystyle}{
    numberstyle=\tiny,
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

% Кафедра (в родительном падеже)
\chair{теории функций и стохастического анализа} 
%\chairr{кафедра теории функций и стохастического анализа} % для практик!!!!
\chairr{механико-математический факультет}% для практик!!!!

% Тема работы

\title{Перевод фрагмента из книги "Machine Learning with R".}

% Курс
\course{3}

% Группа
\group{312}

\department{механико-математического факультета}

% Специальность/направление код - наименование
\napravlenie{01.03.02 "--- Прикладная математика и информатика}
%\napravlenie{38.03.05 "--- Бизнес-информатика}
%\napravlenie{09.04.03 "--- Прикладная информатика}

% Для СТУДЕНТКИ!!!. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Георгиева Ивана Владимировича}

% Заведующий кафедрой
\chtitle{д.\,ф.-м.\,н., доцент} % степень, звание
\chname{С.\,П.\,Сидоров}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{к.\,ф.-м.\,н., доцент} %должность, степень, звание
\saname{О.\,А.\,Мыльцина} % для 451 гр
%\saname{А.\,К.\,Смирнов} % для 412 гр

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
\patitle{д.\,ф.-м.\,н., доцент}
\paname{С.\,П.\,Сидоров}

% Семестр (только для практики, для остальных
% типов работ не используется)
\term{5}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{производственная(базовая)}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{4}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{29.06.2020}
\practFinish{26.07.2020}

% Год выполнения отчета
\date{2020}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations
%\begin{description}
%    \item $|A|$  "--- количество элементов в конечном множестве $A$;
 %   \item $\det B$  "--- определитель матрицы $B$;
 %   \item ИНС "--- Искусственная нейронная сеть;
 %   \item FANN "--- Feedforward Artifitial Neural Network
%\end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro

Целью настоящей работы является перевод книги Абрахама Гхатака "Machine Learning with R", знакомство с машинным обучением, языком программирования R и реализациями некоторых видов регрессий.

Отчет по практике состоит из введения, основной части, содержащей три раздела, и заключения. В первом разделе был переведён параграф 4.7. из книги Гхатака. Во втором - 4.8.В третьем - 4.9. Список использованных источников содержит 5 наименований на которые в тексте приведены ссылки.


\section{Гребневая Регрессия}

Ранее мы видели, что добавление многочленов более высокой степени к уравнению регрессии приводит к переобучению. Переобучение происходит, когда модель слишком хорошо подходит для обучающих данных и не обобщается на ранее неизветсные данные.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_5.png}
    \caption{\label{fig:4.5}
    Трудно переобучить из-за множества наблюдений}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_6.png}
    \caption{\label{fig:4.6}
    Легко переобучить из-за крайне малого числа наблюдений}
\end{figure}

Обратимся к рисункам \ref{fig:4.5} и \ref{fig:4.6}. Переобучение также может произойти, если в уравнении регрессии слишком много независимых переменных или, если наблюдений слишком мало.
Переобучение также связано с очень большими оценочными параметрами (весами) $\hat{w}$.


Поэтому мы хотим найти баланс между
\begin{itemize}
	\item Насколько хорошо наша модель соответствует данным (мерой соответствия)
	\item Величиной коэффициентов
\end{itemize}

Таким образом, общая стоимость модели представляет собой комбинацию меры соответствия и величиной коэффициентов. Мера соответствия представлена суммой квадратов разностей(RSS). Небольшая указывает на хорошее соответствие. Мера величины коэффициентов - это сумма абсолютных значений коэффициентов $l_1$ норма или сумма квадратов значений коэффициентов $l_2$ норм. Они представлены следующим образом:


\begin{equation*}
	\left\|w_0\right\| + \left\|w_1\right\| + \cdots + \left\|w_n\right\| = \sum_{j = 0}^n \left\|w_j\right\| = \left\|w\right\|_1 (l_1 \text{ норма})
\end{equation*}

\begin{equation*}
	w_0^2  + w_1^2  + \cdots + w_n^2  = \sum_{j = 0}^n w_j^2  = \left\|w\right\|_2^2 (l_2 \text{ норма})
\end{equation*}

В гребневой регрессии мы считаем $l_2$ норму как меру величины коэффициентов. Таким образом, общая стоимость

\begin{equation}
\label{eq:4.7.1}
	Total\text{ }Cost = RSS(w) + \left\|w\right\|_2^2
\end{equation}


Наша цель в гребневой регрессии состоит в том, чтобы найти $\hat{w}$, чтобы минимизировать общие затраты в уравнении \ref{eq:4.7.1}. Баланс между мерой соответствия и величиной коэффициентов достигается путем введения параметра настройки $\lambda$ так, чтобы

\begin{equation}
\label{eq:4.7.2}
	Total\text{ }Cost = RSS(w) + \lambda\left\|w\right\|_2^2
\end{equation}

Если $\lambda = 0$ $\rightarrow$ сводится к минимизации $RSS(w)$ $\rightarrow$ $\hat{w}^{Least Squares(LS)}$

Если $\lambda = \infty$ $\rightarrow$ общая стоимость равна $\infty$ когда ($\hat{w} \neq 0$) и общая стоимость равна $0$ когда ($\hat{w} = 0$)
Если $\lambda$ между $\rightarrow$ $\left\|\hat{w}\right\|_2^2 \leq \left\|\hat{w}^{(LS)}\right\|_2^2$

\subsection{Вычисление градиента гребневой регрессии}

\textbf{Закрытая форма решения}

\begin{equation*}
	RSS(w)= (y - \textbf{H}w)^T (y - \textbf{H}w)
\end{equation*}

Общая стоимость в случае гребневой регрессии составляет

\begin{equation}
\label{eq:4.7.3}
\begin{gathered}
	Total\text{ }Cost = (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda\left\|w\right\|_2^2 \\= (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda w^T w
\end{gathered}
\end{equation}

Градиент уравнения \ref{eq:4.7.3}

\begin{equation}
\label{eq:4.7.4}
\begin{gathered}
	\Delta\left[RSS(w) + \lambda\left\|w\right\|_2^2\right] = \Delta (y - \textbf{H}w)^T (y - \textbf{H}w) + \lambda\left\|w\right\|_2^2 \\
	\Delta Cost(w) = -2\textbf{H}^t (y - \textbf{H}w) + \lambda (2w) \\
	= -2\textbf{H}^t (y - \textbf{H}w) + 2 \lambda Iw
\end{gathered}
\end{equation}

Приравнивая уравнение \ref{eq:4.7.4} к 0, получаем

\begin{equation}
\label{eq:4.7.5}
\begin{gathered}
	\Delta Cost(w) = 0\\
	-2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda I w = 0\\
	-\textbf{H}^T y + \textbf{H}^T \textbf{H} \hat{w} + \lambda I \hat{w} = 0\\
	(\textbf{H}^T \textbf{H} + \lambda I) \hat{w} = \textbf{H}^T y\\
	\hat{w}_{ridge} = (\textbf{H}^T \textbf{H} + \lambda I)^{-1} \textbf{H}^T y
\end{gathered}
\end{equation}

Если $\lambda = 0$:$\rightarrow$ $(\hat{w}_{ridge} = \textbf{H}^T \textbf{H})^{-1} \textbf{H}^T y = \hat{w}^{LS}$

Если $\lambda = 0$:$\rightarrow$ $(\hat{w}_{ridge} = 0$

Тот факт, что мы заботимся о сложности модели, называется регуляризацией (см. предыдущие разделы). Сложность модели определяется параметром настройки $\lambda$, который снижает модель (за счет стоимости), если ее сложность увеличивается.

\textbf{Градиентный спуск}
Поэлементный алгоритм градиентного спуска гребневой регрессии можно записать как

\begin{equation}
\label{eq:4.7.6}
\begin{gathered}
	w_j^{(t + 1)} \leftarrow w_j^{(t)} + \Delta Cost(w)\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)} - 2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda w\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta \left[-2 \sum_{i = 1}^n h_j(x_i)(y_i - \hat{y}_i(w^{(t)})) + 2 \lambda w_j^{(t)}\right]\\
	w_j^{(t + 1)} \leftarrow w_j^{(t)}(1 - 2 \eta \lambda) + 2 \eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i(w^{(t)}))
\end{gathered}
\end{equation}

В уравнении \ref{eq:4.7.6} $\eta$ - размер шага и выражение $(1 - 2 \eta\lambda)$ всегда меньше или равно 1 пока $\eta > 0$, $\lambda > 0$. Выражение $2\eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i(w^{(t)}))$ - коэффициент обновления из RSS

На рисунке \ref{fig:4.7} показано, что $w_j^{(t)}$ уменьшается в $(1 - 2 \eta\lambda)$ на промежуточном этапе, а затем $w_j^{(t + 1)}$ увеличивается в коэффициент обновления $2\eta \sum_{i = 1}^n h_j(x_i) (y_i - \hat{y}_i (w^{(t)}))$ 

Обобщая,
\begin{itemize}
	\item Для регрессии методом наименьших квадратов: $w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta *$ (коэффициент обновления)
	\item Для гребневой регрессии: $w_j^{(t + 1)} \leftarrow (1 - 2\eta\lambda)w_j^{(t)} - \eta *$ (коэффициент обновления)
\end{itemize}

Мы всегда можем лучше подобрать обучающую выборку со сложной моделью и настройкой спада веса $\lambda = 0$, чем мы могли бы с менее сложной моделью и положительным спадом веса. Это подводит нас к вопросу о том, как выбрать параметр настройки $\lambda$? Чтобы найти $\lambda$, мы используем $k$-кратную перекрестную проверку (см. предыдущие разделы).

Процесс включает подгонку $\hat{w}_{\lambda}$ к обучающему набору, тестирование производительности модели с $\hat{w}_{\lambda}$ на проверочном наборе для выбора $\lambda^*$ и, наконец, оценку ошибки обобщения модели с $\hat{w}_{\lambda^*}$. Средняя ошибка вычисляется следующим образом

\begin{equation}
\label{eq:4.7.7}
\begin{gathered}
	\text{Средняя ошибка } CV(\lambda) = \frac{1}{k} \sum_{k = 1}^k error_k (\lambda)
\end{gathered}
\end{equation}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_7.png}
    \caption{\label{fig:4.7}
    Регуляризация весов при гребневой регрессии}
\end{figure}

\subsection{Написание приложения градиентного спуска гребневой регрессии}

Теперь мы напишем наше собственное приложение гребневой регрессии с нуля, и мы будем использовать $kc\_house\_data$, который использовался ранее.
Давайте сначала воспользуемся функцией "lm" в R и извлечем коэффициенты, регрессируя $price$ с $sqft\_living$ и $bedrooms$, из $house\_train\_data$.

\lstinputlisting[language=R]{code_1.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms \\
97050.0942 & 305.2103 & -57429.9302 \\
\end{tabular}
\end{center}

Первая функция в нашем приложении градиентного спуска гребневой регрессии - это создание матрицы данных.

\lstinputlisting[language=R]{code_2.txt}

Функция predict\_output предсказывает целевые значения.

\lstinputlisting[language=R]{code_3.txt}

Штраф $l_2$ получил свое название, потому что он заставляет веса иметь меньшую $l_2$ норму, чем в противном случае.
Следующая функция принимает начальные веса и матрицу признаков и предсказывает целевые значения с помощью функции predict\_output.
errors находятся по разнице в предсказанных и фактических значениях.
Производная или gradient вычисляется с использованием матрицы признаков и errors $(\Delta cost(w) = 2 \textbf{H}^T (y - \textbf{H} w) + 2 \lambda w)$.
Веса обновляются путем вычитания произведения gradient и step\_size (см. уравнение \ref{eq:4.7.6}.
Норма градиентов рассчитывается (см. уравнение выше), чтобы проверить, меньше ли она tolerance в цикле «while».

\lstinputlisting[language=R]{code_4.txt}

Мы присвоим два разных значения параметру настройки $\lambda - 0$ и $\infty$ и исследуем, как параметры (веса) корректируются. Мы рассмотрим модель с двумя функциями, то есть sqft\_living и bedrooms.

\lstinputlisting[language=R]{code_5.txt}

\begin{center}
\begin{tabular}{c}
[1] "Gradient descent converged at iteration: 3002" \\
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_6.txt}

\begin{center}
\begin{tabular}{c}
$\left[,1\right]$\\
$\left[1,\right] -5.560047e-17$\\
$\left[2,\right] 7.885384e-01$\\
$\left[3,\right] -1.489809e-01$\\
\end{tabular}
\end{center}

Коэффициенты с $\lambda = 0$ такие же, как для OLS регрессии. Давайте проверим, получим ли мы те же результаты, используя функцию «lm» в R.

\lstinputlisting[language=R]{code_7.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms \\
2.600634e-17 & 7.885384e-01 & -1.489809e-01 \\
\end{tabular}
\end{center}

Действительно получим!
R имеет функцию «glmnet» в пакете glmnet. Характеристики и результат принадлежат классу matrix. Аргумент «alpha» равен 0 для гребневой регрессии гребня и 1 для регрессии лассо. Допуск, определенный в нашем алгоритме, - это «порог», который по умолчанию равен 1e-07. Есть много других параметров, которые можно изучить, набрав help(glmnet).
Давайте проверим наши результаты с помощью функции «glmnet» в R (Таблица \ref{table:4.7}).

\lstinputlisting[language=R]{code_8.txt}

\begin{table}[!ht]
\caption{\label{table:4.7}Сравнение коэффициентов гребневой регрессии с $\lambda = 0$ для нашего приложения, модели регрессии OLS и glmnet}
\begin{center}
\begin{tabular}{c|c|c|c}
\hline
             &  Application &  OLS     &  glmnet \\
\hline
(Intercept)  &  0.00000     &  0.00000 &  0.00000\\
\hline
sqft\_living &  0.78854     &  0.78854 &  0.78853\\
\hline
bedrooms     & -0.14898     & -0.14898 & -0.14898\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[!ht]
\caption{\label{table:4.8}Сравнение коэффициентов гребневой регрессии с $\lambda = 1e+5$ для нашего приложения и glmnet}
\begin{center}
\begin{tabular}{c|c|c}
\hline
             & Application& glmnet\\
\hline                     
(Intercept)  & 0.00000    & 0e+00 \\
\hline                     
sqft\_living & 0.06679    & 1e-05 \\
\hline                     
bedrooms     & 0.02696    & 0e+00 \\
\hline
\end{tabular}
\end{center}
\end{table}

\lstinputlisting[language=R]{code_9.txt}

Действительно, наше приложение хорошо сравнивает!
Теперь давайте попробуем наше приложение с $\lambda = 1e+05$

\lstinputlisting[language=R]{code_10.txt}

Эта модель имеет очень низкую предвзятость.
Давайте проверим наши результаты с помощью функции glmnet в R (таблица \ref{table:4.8}).

\lstinputlisting[language=R]{code_11.txt}

Действительно, мы получаем одинаковые результаты, т.е. все коэффициенты очень близки к $0$!
Представим себе уравнения гребневой регрессии для двух значений $\lambda$ на рис. $\ref{fig:4.8}$. Модель гребневой регрессии с градиентным спуском переходит в нулевую модель с отсечением 0 (высокое смещение), когда $\lambda = \infty$, и модель регрессии LS, когда $\lambda = 0$.
Наша следующая цель - найти наилучшее значение $\lambda$; и мы будем использовать перекрестную проверку, как описано в предыдущих разделах.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_8.png}
    \caption{\label{fig:4.8}
    Гребневая регрессия с $\lambda = 0$ и $\lambda = 1e+05$}
\end{figure}

\section{Оценка производительности}

Мы говорили о стоимости модели в разных разделах. По сути, стоимость модели - это мера ошибок модели, то есть того, насколько точно модель может предсказать фактическое значение. То есть, если модель не допускает ошибок прогнозирования, стоимость, приписываемая модели, равна нулю, и, следовательно, ее мера точности составляет $100 \% $.
Ранее мы видели, что стоимость регрессионной модели измеряется функцией потерь, определяемой как

\begin{equation}
\label{eq:4.8.1}
\begin{gathered}
	Loss(y, f_{\hat{w}}(x)) = (y - f_{\hat{w}}(x))^2 - \text{ошибка в квадрате}\\
	Loss(y, f_{\hat{w}}(x)) = \left|y - f_{\hat{w}}(x)\right| - \text{модуль ошибки}
\end{gathered}
\end{equation}

О мере точности модели можно судить по стоимости модели на данных обучения и невидимых данных. Мы видели в предыдущих разделах, что при обучении модели модель пытается соответствовать обучающим данным. Следовательно, с увеличением сложности модели ошибка обучения уменьшается. Очень низкая ошибка обучения не является хорошим показателем точности модели, если обучающие данные не включают «вселенную», то есть «видимые» и
«невидимые» данные.
Следовательно, эффективность модели можно оценить только по степени ее точности с «невидимыми» данными.
\textbf{Ошибка обобщения} - это ошибка модели из «вселенной» «невидимых» данных. \textbf{Истинная ошибка} - это ошибка из доступного подмножества, из «вселенной» «невидимых» данных.
Обсуждаемые здесь ошибки модели зависят от сложности и других алгоритмических характеристик модели. Но в самих данных могут быть ошибки. В следующем разделе мы попытаемся выяснить, как мы можем решить эту проблему.

\subsection{Еще раз об источниках ошибок}

Мы обсудили, как ошибки могут возникать из данных обучения модели и из невидимых данных. Однако существуют также три других источника ошибок в модели, как описано в одном из предыдущих разделов - шум, смещение и дисперсия.
\textbf{Шум} всегда проникает в данные, потому что мы не можем точно записать то же самое, скажем, «человеческие эмоции», «проблемы во взаимоотношениях», «глобальные проблемы» и т. Д.
На основании имеющихся данных в наши модели также проникают \textbf{смещения} и \textbf{отклонения}. Если мы обучим модель на основе обучающих данных, в которых нет шума(noise), и оценим ту же модель на невидимых данных, которые содержат шум, мы получим модель с очень низкой точностью.
Следовательно, как описано в одном из предыдущих разделов, ошибка прогнозирования в модели состоит из трех компонентов: шума, смещения и дисперсии, которые можно записать как

\begin{equation}
\label{eq:4.8.2}
\begin{gathered}
	Prediction Error = \varsigma^2 + MSE\left[f_{\hat{w}}(x)\right]\\
	= \varsigma^2 + \left[bias(f_{\hat{w}}(x))\right]^2 + var(f_{\hat{w}}(x))
\end{gathered}
\end{equation}

Подходящая модель выбирается путем выбора параметра настройки $\lambda$, который управляет сложностью модели. Это делается с помощью перекрестной проверки. Выбрав подходящую модель, оцениваем ошибку обобщения.
Давайте рассмотрим house\_train\_data и house\_test\_data, которые мы разделили
из набора данных kc\_house. Наборы данных для обучения и тестирования имеют 10 807 и 10 806
наблюдения соответственно.

\lstinputlisting[language=R]{code_12.txt}

Мы обучаем модель регрессии гребня с $\lambda = 0$, используя наши обучающие данные, т. е. реплицируем OLS регрессию и используем нашу модель для прогнозирования невидимых тестовых данных с $\lambda = 10$.

\lstinputlisting[language=R]{code_13.txt}

\begin{center}
\begin{tabular}{c}
[1] "MSE\_lambda\_10 = 0.903155"
\end{tabular}
\end{center}

Среднеквадратичная ошибка для этой модели составляет $0.903155$.
R имеет встроенную функцию перекрестной проверки cv.glmnet(). По умолчанию функция выполняет 10-кратную перекрестную проверку, но это можно изменить с помощью аргумента «nfolds».
Мы выполняем 10-кратную перекрестную проверку данных обучения и находим значение $\lambda$, для которого ошибка перекрестной проверки наименьшая.

\lstinputlisting[language=R]{code_14.txt}

\begin{center}
\begin{tabular}{c}
[1] "ideal\_lambda = 0.076901"
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_15.txt}

\begin{center}
\begin{tabular}{c}
[1] "MSE with ideal\_lambda = 0.495869"
\end{tabular}
\end{center}

Среднеквадратичная ошибка при $\lambda = 0.076901$ составляет $0.495869$, т. е. MSE имеет
упала на $45\%$.
Выясним коэффициенты этой модели

\lstinputlisting[language=R]{code_16.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept)   & sqft\_living & bedrooms\\
-1.397232e-16 & 6.998124e-01 & -8.978171e-02
\end{tabular}
\end{center}

Давайте визуализировать это уравнение гребневой регрессии с помощью предиктора sqft\_living.
На рис. \ref{fig:4.9} представлено уравнение регуляризованной гребневой регрессии.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_9.png}
    \caption{\label{fig:4.9}
    Гребневая регрессия с $\lambda = 0.076901$ полученное с перекрестной проверкой}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_10.png}
    \caption{\label{fig:4.10}
    Стандартизированные коэффициенты регрессии Риджа при изменении $\lambda$. Коэффициенты не падают резко до 0 при $\lambda \rightarrow \infty$}
\end{figure}

Представим себе, как меняются стандартизированные коэффициенты при разных значениях $\lambda$. На рис. \ref{fig:4.10} видно, что коэффициенты меняются постепенно от $\lambda = 0$ до $\lambda \rightarrow \infty$. Он никогда резко не уменьшается до 0.

\subsection{Компромисс смещения – дисперсии в гребневой регрессии}

Большое значение $\lambda$ приведет к высокому смещению (простая модель) и, следовательно, к низкой дисперсии $(\hat{w} = 0$, для $\lambda = \infty)$.
Небольшое значение $\lambda$ приведет к низкому смещению и, как следствие, большой дисперсии.

\section{Лассо-регрессия}

Если у нас есть «широкий» набор функций (скажем, $1e+10$), гребневая регуляризация может создать вычислительные проблемы, поскольку выбраны все функции.
Алгоритм лассо отбрасывает менее важные / избыточные характеристики, переводя их коэффициенты в ноль. Это позволяет нам интерпретировать функции, а также сокращает время вычислений. Лассо (регуляризованная регрессия $l_1$) использует норму $l_1$ в качестве штрафа,
вместо нормы $l_2$ в гребневой регрессии.
Прежде чем мы перейдем к целевой функции лассо, давайте вернемся к алгоритму гребневой регрессии. Гребневая регрессия выбирает параметры $\beta$ с минимальной RSS при условии, что норма $l_2$ параметров $\beta_1^2 + \beta_2^2 + \cdots + \beta_n^2 \leq t$, ограничение гребня.
На рисунке \ref{fig:4.11} показаны изолинии RSS и ограничения параметров для гребневой регрессии и регрессии лассо.
Контуры RSS представляют собой эллипсы, центрированные по методу наименьших квадратов (точка красного цвета). Ограничение построено для различных значений параметров $\beta$. В случае гребневой регрессии $\beta_1^2 + \beta_2^2 \leq t$, ограничение гребня. Ограничение принимает форму круга для двух параметров и становится сферой с большим количеством параметров. Первая точка соприкосновения контура RSS с окружностью - это точка, описывающая параметры гребня $\beta_1$ и $\beta_2$. Значения $\beta$ в большинстве случаев оказываются ненулевыми. Если $t$ мало, параметры будут маленькими, а если велико, будет стремиться к решению по методу наименьших квадратов.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_11.png}
    \caption{\label{fig:4.11}
    Левый график показывает контуры RSS гребневой регрессии и ограничение на коэффициенты гребня $\beta_1^2 + \beta_2^2 \leq t$ (ограничение гребня). Правый график показывает RSS-контуры регрессии лассо и ограничение на коэффициенты лассо $\left|\beta_1\right| + \left|\beta_2\right| \leq t$ (ограничение лассо)}
\end{figure}

В случае регрессии лассо параметры $\beta$ выбираются такими, что $\left|\beta_1\right| + \left|\beta_2\right| \leq t$, ограничение лассо, для минимального RSS.
Как показано на правом графике рис. \ref{fig:4.11} для регрессии лассо с двумя параметрами, ограничение имеет форму ромба с четырьмя углами; для более чем двух элементов контур становится ромбовидным с множеством углов. Если контур RSS касается угла, он заставляет одну из $\beta$ стать нулевой.
Мы можем переписать общую стоимость в формуле \ref{eq:4.7.3}, для регрессии лассо как

\begin{equation}
\label{eq:4.9.1}
\begin{gathered}
	Total\text{ }Cost = RSS(w) + \lambda \left\|w\right\|_1
\end{gathered}
\end{equation}

Если $\lambda = 0 \rightarrow \hat{w}^{lasso} = \hat{w}^{Least Squares}$
Если $\lambda = \infty \rightarrow \hat{w}^{lasso} = 0$
Если $\lambda$ между $\rightarrow 0 \leq \hat{w}^{lasso} \leq \hat{w}^{Least Squares}$

Лассо выбирает параметры $\beta$ с минимальным RSS, при условии, что $l_1$ норма параметров $(\left|\beta_1\right| + \left|\beta_2\right| + \cdots + \left|\beta_n\right|) \leq tolerance$.
Ранее мы видели, что оптимальное решение задачи минимизации лассо находится в начале координат, и поэтому мы не можем вычислить градиент.
Поэтому решением является выпуклый алгоритм оптимизации, называемый Координатным спуском. Этот алгоритм пытается минимизировать

\begin{equation}
\label{eq:4.9.2}
\begin{gathered}
	f(w) = f(w_0, w_1, \cdots, w_n)\\
	find \underset{min}{f}(w)
\end{gathered}
\end{equation}

Алгоритм координатного спуска можно описать следующим образом:

\begin{equation}
\label{eq:4.9.3}
\begin{gathered}
	\text{Initialize } \hat{w}\\
	\text{while not converged, pick a coordinate j}\\
	\hat{w}_j \leftarrow \underset{min}{f}(\hat{w}_0. \hat{w}_1, \cdots, w, \hat{w}_{j + 1}, \cdots, \hat{w}_n)
\end{gathered}
\end{equation}

Если мы выберем следующую координату случайным образом, это станет стохастическим координатным спуском.
При координатном спуске нам не нужно выбирать размер шага.

\subsection{Координатный спуск для регрессии наименьших квадратов}

Ниже приведен алгоритм спуска координат для LS, по одной координате за раз.

\begin{equation}
\label{eq:4.9.4}
\begin{gathered}
RSS(w) = \sum_{i = 1}^n (y_i - \sum_{j = 0}^n w_j h_j(x_i))^2\\
\text{Зафиксируем все координаты $w_{-j}$ и возьмем частную производную по $w_j$}\\
\frac{\partial}{\partial w_j} = -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{j = 0}^n w_j h_j(x_i))^2\\
= -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{k \neq j}w_k h_k(x_i) - w_j h_j(x_i))\\
= -2 \sum_{i = 1}^m h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i)) + 2 w_j \sum_{i = 1}^n h_j(x_i)^2\\
\text{Пояснения:}\\
\text{(i) по определению нормированных функций,} \sum_{i = 1}^n h_j(x_i)^2 = 1\\
\text{(ii) мы будем обозначать} (\sum_{i = 1}^n h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i)) = \rho_j\\
= -2 \rho + 2 w_j\\
\text{приравняв частичные производные к 0, получим}\\
w_j = \rho_j
\end{gathered}
\end{equation}

\subsection{Координатный спуск для регрессии лассо}

Ниже приведен псевдокод спуска координат для регрессии лассо, по одной координате за раз.


\begin{equation}
\label{eq:4.9.5}
\begin{gathered}
\text{Initialize } \hat{w}\\
\text{while not converged}\\
\text{for j in } 0,1,2, \cdots,n\\
\text{compute: } \rho_j = \sum_{i = 1}^n h_j(x_i)(y_i ? \hat{y}_i(\hat{w}_j))\\
\text{set: } w_j = \begin{cases}
\rho_j + \frac{\lambda}{2} \text{ if } \rho_j < - \frac{\lambda}{2}\\
0 \text{ if } \rho_j \text{ in } \left[-\frac{\lambda}{2}, \frac{\lambda}{2}\right]\\
\rho_j - \frac{\lambda}{2} \text{ if } \rho_j > \frac{\lambda}{2}
\end{cases}\\
\end{gathered}
\end{equation}

\subsection{Написание приложения для координатного спуска регрессии лассо}

Теперь мы готовы написать алгоритм регрессии лассо.
Функция get\_features подбирает данные на основе выбранных функций.

\lstinputlisting[language=R]{code_17.txt}

Функция add\_constant\_term добавляет столбец единиц к матрице признаков.

\lstinputlisting[language=R]{code_18.txt}

Функция construct\_matrix принимает имена функций и выходные данные из data.frame. Затем он выбирает значения функций и выходные значения, вызывая функцию get\_features, и возвращает их в виде списка.

\lstinputlisting[language=R]{code_19.txt}

Функция normalize\_features вычисляет норму для каждой функции и нормализует feature\_matrix.

\lstinputlisting[language=R]{code_20.txt}

Функция predict\_output предсказывает выходные значения.

\lstinputlisting[language=R]{code_21.txt}

В уравнении \ref{eq:4.9.4} было показано, что $\rho_j = \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{k \neq j} w_k h_k(x_i))$. Для понимания последующего кода, в котором мы вычисляем $\rho$, давайте рассмотрим набор данных с 3 функциями, и мы хотим найти $\rho_2$. Это можно записать как


\begin{equation}
\label{eq:4.9.6}
\begin{gathered}
\rho_2 = h_2(x_2) \left[(y_1 + y_2 + y_3) - (\hat{w}_1 h_1(x_1) + \hat{w}_3 h_3(x_3))\right]\\
= h_2(x_2) \left[(y_1 + y_2 + y_3) - (\hat{y}_1 + \hat{y}_3)\right]\\
= h_2(x_2) \left[(y_1 - \hat{y}_1) + (y_2 - \hat{y}_2) + (y_3 - \hat{y}_3) + \hat{y}_2\right]\\
= h_2(x_2) \left[(target - predicted) + \hat{w}_2 h_2(x_2)\right]\\
\end{gathered}
\end{equation}

\lstinputlisting[language=R]{code_22.txt}

Теперь мы реализуем функцию координатного спуска, которая минимизирует функцию стоимости для одного признака i. Функция возвращает новый вес для признака i.

\lstinputlisting[language=R]{code_23.txt}

Теперь, когда у нас есть функция, которая оптимизирует функцию стоимости по одной координате, давайте реализуем циклический спуск координат, где мы оптимизируем координаты от 1 до n по порядку и повторяем.

\lstinputlisting[language=R]{code_24.txt}

\subsection{Реализация координатного спуска}

Мы будем использовать полный набор данных KC house, состоящий из 21613 наблюдений, и разделим его на тренировочные и тестовые данные, состоящие из 19451 и 2162 наблюдений соответственно.
Мы будем использовать две функции, а именно «sqft\_living» и «bedrooms», и выполним нашу функцию lasso\_cyclical\_coordinate\_descent со штрафом $l_1 = 0$, что аналогично регрессии LS.

\lstinputlisting[language=R]{code_25.txt}

\begin{center}
\begin{tabular}{c}
[1] 97050.0942 305.2103 -57429.9302
\end{tabular}
\end{center}

Сравним наши результаты с функциями glm и lm в R.

\lstinputlisting[language=R]{code_26.txt}

\begin{center}
\begin{tabular}{c}
[1] 97052.1826 305.1896 -57417.7511
\end{tabular}
\end{center}

\lstinputlisting[language=R]{code_27.txt}

\begin{center}
\begin{tabular}{ccc}
(Intercept) & sqft\_living & bedrooms\\
97050.0942  & 305.2103    & -57429.9302
\end{tabular}
\end{center}

Действительно, наш алгоритм лассо очень хорошо сравнивается и дает те же результаты!

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{fig_4_12.png}
    \caption{\label{fig:4.12}
    Коэффициенты регрессии Лассо при изменении $\lambda$. Коэффициенты резко падают до 0 при $\lambda \rightarrow \infty$
    }
\end{figure}

В лассо, когда $\lambda = 0$, решение лассо является методом наименьших квадратов, а когда $\lambda$ становится достаточно большим, лассо дает нулевую модель, в которой все оценки коэффициентов равны нулю.
На рис. \ref{fig:4.12} мы построили 7 коэффициентов лассо при изменении $\lambda$ от 0 до $1e+10$. Можно заметить, что между двумя крайностями подгонки LS и нулевой модели, в зависимости от значения $\lambda$, лассо создает модели с любым количеством переменных, и большинство из них отклоняются по мере увеличения значения $\lambda$.

\subsection{Компромисс отклонения отклонения в регрессии лассо}

Большое значение $\lambda$ приведет к высокому смещению (простая модель) и, следовательно, к низкой дисперсии($\hat{w} = 0$, для $\lambda = \infty$).
Небольшое значение $\lambda$ приведет к низкому смещению и, как следствие, большой дисперсии.
Мы подробно обсудили регрессию; Теперь мы перейдем к следующей области обучения с учителем - классификации.

% Раздел "Заключение"
\conclusion
В ходе данной работы был успешно переведён отрывок из книги ~\cite{Machine_Learning_with_R}. Для этого были использованы англо-русские словари ~\cite{dict1}, ~\cite{dict2}, ~\cite{dict3}, ~\cite{dict4}.

Мы познакомились с современными методами перевода технической литературы.

Была проведена работа с системой Google как в целях поиска информации, так и во время работы с переводом.

Было проведено знакомство с множеством различных англо-русских словарей.

Мы познакомились с методами реализации на языке R следующих понятий:

\begin{itemize}
	\item Гребневая регрессия
	\item Градиентный спуск
	\item Регрессия лассо
	\item Координатный спуск
\end{itemize}

С помощью платформы Kaggle была проведена попытка проверить работоспособность кода, приведённого в книге в разделах 4.7 - 4.9. Kaggle выдал ошибку компиляции, а значит, код оказался нерабочим.


%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
%\bibliographystyle{gost780uv}
%\bibliographystyle{ugost2008}
%\bibliographystyle{ugost2008_In} как в каталоге
%\bibliographystyle{utf8gost780u} фамилии курсивом
%\bibliographystyle{utf8gost71u} %фамилии рядом
%\bibliographystyle{ugost2003s}


%\bibliography{thesis}



\begin{thebibliography}{99}
  \bibitem{Machine_Learning_with_R} Ghatak, A. Machine Learning with R / A. Ghatak. - Singapore: Springer Nature, 2017. - 210 pp.
  \bibitem{dict1} Борковский, А.Б. Англо-русский словарь по программированию и информатике / Борковский, А.Б., - СССР: Русский язык, 1987. - 336 стр.
  \bibitem{dict2} Борковский, А.Б., Зайчик, Б.И. Словарь по программированию (английский, русский, немецкий, французский) / Зайчик, Б.И., - СССР: Русский язык, 1991. - 286 стр.
  \bibitem{dict3} Александров, П.С., Ловатер, А.Дж. Англо-русский словарь математических терминов / Александров, П.С., Ловатер, А.Дж., - Россия: Мир, 2001. - 416 стр.
  \bibitem{dict4} Мюллер, В.К.Самый полный англо-русский русско-английский словарь с современной транскрипцией: около 500 000 слов / Мюллер, В.К., - Россия: АСТ, 2016. - 800 стр.
\end{thebibliography}

\end{document}
