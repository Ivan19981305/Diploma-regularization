\documentclass[bachelor, och, autoref, times]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа

				%    pract2      - отчет по практике,  2 курс
				%    pract3      - отчет по практике,  3 курс
				%    pract4      - отчет по практике,  4 курс
%    nir      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
%\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}

% выделение ссылок цветом. чтобы включить- true
\usepackage[colorlinks=false]{hyperref}

\renewcommand{\rmdefault}{cmr} % курсив и полужирный включаются здесь.
\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}


\begin{document}

% Кафедра (в родительном падеже)
\chair{теории функций и стохастического анализа} 
%\chairr{кафедра теории функций и стохастического анализа} % для практик!!!!
\chairr{завод "Тантал"}% для практик!!!!

% Тема работы

\title{Использование регуляризации для решения проблемы переобучения}

% Курс
\course{4}

% Группа
\group{412}

\department{механико-математического факультета}

% Специальность/направление код - наименование
\napravlenie{01.03.02 "--- Прикладная математика и информатика}
%\napravlenie{38.03.05 "--- Бизнес-информатика}
%\napravlenie{09.04.03 "--- Прикладная информатика}

% Для СТУДЕНТКИ!!!. Для работы студента следующая команда не нужна.
%\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Георгиева Ивана Владимировича}

% Заведующий кафедрой
\chtitle{д.\,ф.-м.\,н., доцент} % степень, звание
\chname{С.\,П.\,Сидоров}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{д.\,ф.-м.\,н., доцент} %должность, степень, звание
\saname{С.\,П.\,Сидоров} % для 451 гр
%\saname{А.\,К.\,Смирнов} % для 412 гр

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
\patitle{ведущий программист}
\paname{Д.\,Э.\,Кнутов}

% Семестр (только для практики, для остальных
% типов работ не используется)
\term{5}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{производственная(базовая)}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{4}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
\practStart{29.06.2019}
\practFinish{26.07.2019}

% Год выполнения отчета
\date{2021}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering

% Содержание
%\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations
%\begin{description}
%    \item $|A|$  "--- количество элементов в конечном множестве $A$;
 %   \item $\det B$  "--- определитель матрицы $B$;
 %   \item ИНС "--- Искусственная нейронная сеть;
 %   \item FANN "--- Feedforward Artifitial Neural Network
%\end{description}

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro
\textbf{Актуальность темы}. В современном мире машинное обучение приобретает популярность при решении обширного класса задач. Практически в каждой из таких задач возникает проблема переобучения. При переобучении построенная для решения поставленной задачи модель может давать ответы, близкие к реальным, на данных, используемых для обучения. Но при этом на остальных данных результаты могут значительно отличаться от ожидаемых.

Для решения этой проблемы используется много различных методов. Одним из наиболее актуальных является регуляризация. Так, в популярных библиотеках, предназначенных для машинного обучения, некоторые модели используют регуляризацию автоматически, при условии, что значение соответствующего параметра не предполагает обратного. Переобучение часто наступает при слишком больших значениях некоторых коэффициентов модели. Регуляризация не позволяет модели чересчур увеличивать коэффициенты, то есть снижает уровень переобучения.

\textbf{Целью бакалаврской работы} является исследование регуляризации для решения проблемы переобучения и сравнение результатов обучения модели с использованием и без применения регуляризации.

\textbf{Объект исследования} -- виды регуляризации.

\textbf{Предмет исследования} -- $L_1$ - регуляризация, $L_2$ - регуляризация.

Для достижения поставленных целей в работе необходимо решить следующие \textbf{задачи}:

\begin{itemize}
\item определить основные понятия машинного обучения
\item показать случаи переобучения на примерах
\item рассмотреть основные виды регуляризации
\item собрать данные для обучения модели из открытых источников
\item обучить модель с использованием и без использования регуляризации
\item провести сравнение результатов обучения
\end{itemize}

\textbf{Практическая значимость} данной работы состоит в том, что модель, обученная с применением регуляризации, даёт более точный результат чем модель, которую обучали без использования данного метода. Кроме того полученная модель может быть использована для предсказания диагнозов пациентов по их симптомам. Полученные с её помощью предсказания могут дать направление для дальнейших исследований здоровья больного.

\textbf{Структура и содержание бакалаврской работы}. Работа состоит из введения, трёх разделов, заключения, списка использованных источников, содержащего 20 наименований и приложения. Общий объём работы составляет 45 страниц.

\centerline{\textbf{ОСНОВНОЕ СОДЕРЖАНИЕ РАБОТЫ}}

Во \textbf{введении} обосновывается актуальность темы работы, формулируется цель работы и решаемые задачи, отмечается практическая значимость полученных результатов.

В \textbf{первом разделе} подробно описываются гребневая регрессия и регрессия лассо.

Регрессия Лассо, или регуляризация через манхэттенское расстояние, или $L_1$-регуляризация:
\begin{equation*}
L_1 = \sum_i (y_i - y(t_i))^2 + \lambda \sum_i |a_i|
\end{equation*}

Гребневая регрессия, или регуляризация Тихонова, или $L_2$-регуляризация:
\begin{equation*}
L_2 = \sum_i (y_i - y(t_i))^2 + \lambda \sum_i a_i^2
\end{equation*}

Добавление многочленов более высокой степени к уравнению регрессии приводит к переобучению. Переобучение происходит, когда модель слишком хорошо подходит для обучающих данных и не обобщается на ранее неизвестные данные.

Переобучение также может произойти, если в уравнении регрессии слишком много независимых переменных или, если наблюдений слишком мало.
Переобучение также связано с очень большими оценочными параметрами (весами) $\hat{w}$.
Поэтому мы хотим найти баланс между
\begin{itemize}
	\item Насколько хорошо наша модель соответствует данным (мерой соответствия)
	\item Величиной коэффициентов
\end{itemize}

Таким образом, общая стоимость модели представляет собой комбинацию меры соответствия и величиной коэффициентов. Мера соответствия представлена суммой квадратов разностей($RSS$). Небольшая указывает на хорошее соответствие. Мера величины коэффициентов - это сумма абсолютных значений коэффициентов $l_1$ норма или сумма квадратов значений коэффициентов $l_2$ норм. Они представлены следующим образом:

\begin{equation*}
	\left\|w_0\right\| + \left\|w_1\right\| + \cdots + \left\|w_n\right\| = \sum_{j = 0}^n \left\|w_j\right\| = \left\|w\right\|_1 (l_1 \text{ норма})
\end{equation*}

\begin{equation*}
	w_0^2  + w_1^2  + \cdots + w_n^2  = \sum_{j = 0}^n w_j^2  = \left\|w\right\|_2^2 (l_2 \text{ норма})
\end{equation*}

В гребневой регрессии мы считаем $l_2$ норму как меру величины коэффициентов. Таким образом, общая стоимость

\begin{equation}
\label{eq:4.7.1}
	Total\text{ }Cost = RSS(w) + \left\|w\right\|_2^2
\end{equation}

Наша цель в гребневой регрессии состоит в том, чтобы найти $\hat{w}$, чтобы минимизировать общие затраты в уравнении \ref{eq:4.7.1}. Баланс между мерой соответствия и величиной коэффициентов достигается путем введения параметра настройки $\lambda$ так, чтобы

\begin{equation}
\label{eq:4.7.2}
	Total\text{ }Cost = RSS(w) + \lambda\left\|w\right\|_2^2
\end{equation}

Обобщая,
\begin{itemize}
	\item Для регрессии методом наименьших квадратов: $w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta *$ (коэффициент обновления)
	\item Для гребневой регрессии: $w_j^{(t + 1)} \leftarrow (1 - 2\eta\lambda)w_j^{(t)} - \eta *$ (коэффициент обновления)
\end{itemize}

Мы всегда можем лучше подобрать обучающую выборку со сложной моделью и настройкой спада веса $\lambda = 0$, чем мы могли бы с менее сложной моделью и положительным спадом веса. Это подводит нас к вопросу о том, как выбрать параметр настройки $\lambda$? Чтобы найти $\lambda$, мы используем $k$-кратную перекрестную проверку.

Процесс включает подгонку $\hat{w}_{\lambda}$ к обучающему набору, тестирование производительности модели с $\hat{w}_{\lambda}$ на проверочном наборе для выбора $\lambda^*$ и, наконец, оценку ошибки обобщения модели с $\hat{w}_{\lambda^*}$. Средняя ошибка вычисляется следующим образом

\begin{equation}
\label{eq:4.7.7}
\begin{gathered}
	\text{Средняя ошибка } CV(\lambda) = \frac{1}{k} \sum_{k = 1}^k error_k (\lambda)
\end{gathered}
\end{equation}

Обобщая,
\begin{itemize}
	\item Для регрессии методом наименьших квадратов: $w_j^{(t + 1)} \leftarrow w_j^{(t)} - \eta *$ (коэффициент обновления)
	\item Для гребневой регрессии: $w_j^{(t + 1)} \leftarrow (1 - 2\eta\lambda)w_j^{(t)} - \eta *$ (коэффициент обновления)
\end{itemize}

Мы всегда можем лучше подобрать обучающую выборку со сложной моделью и настройкой спада веса $\lambda = 0$, чем мы могли бы с менее сложной моделью и положительным спадом веса. Это подводит нас к вопросу о том, как выбрать параметр настройки $\lambda$? Чтобы найти $\lambda$, мы используем $k$-кратную перекрестную проверку (см. предыдущие разделы).

Процесс включает подгонку $\hat{w}_{\lambda}$ к обучающему набору, тестирование производительности модели с $\hat{w}_{\lambda}$ на проверочном наборе для выбора $\lambda^*$ и, наконец, оценку ошибки обобщения модели с $\hat{w}_{\lambda^*}$. Средняя ошибка вычисляется следующим образом

\begin{equation}
\label{eq:4.7.7}
\begin{gathered}
	\text{Средняя ошибка } CV(\lambda) = \frac{1}{k} \sum_{k = 1}^k error_k (\lambda)
\end{gathered}
\end{equation}

Если у нас есть «широкий» набор функций (скажем, $1e+10$), гребневая регуляризация может создать вычислительные проблемы, поскольку выбраны все функции. Алгоритм лассо отбрасывает менее важные / избыточные характеристики, переводя их коэффициенты в ноль. Это позволяет нам интерпретировать функции, а также сокращает время вычислений. Лассо (регуляризованная регрессия $l_1$) использует норму $l_1$ в качестве штрафа, вместо нормы $l_2$ в гребневой регрессии.

Прежде чем мы перейдем к целевой функции лассо, давайте вернемся к алгоритму гребневой регрессии. Гребневая регрессия выбирает параметры $\beta$ с минимальной RSS при условии, что норма $l_2$ параметров $\beta_1^2 + \beta_2^2 + \cdots + \beta_n^2 \leq t$, ограничение гребня.

В случае гребневой регрессии $\beta_1^2 + \beta_2^2 \leq t$, ограничение гребня. Ограничение принимает форму круга для двух параметров и становится сферой с большим количеством параметров. Первая точка соприкосновения контура RSS с окружностью - это точка, описывающая параметры гребня $\beta_1$ и $\beta_2$. Значения $\beta$ в большинстве случаев оказываются ненулевыми. Если $t$ мало, параметры будут маленькими, а если велико, будет стремиться к решению по методу наименьших квадратов.

В случае регрессии лассо параметры $\beta$ выбираются такими, что $\left|\beta_1\right| + \left|\beta_2\right| \leq t$, ограничение лассо, для минимального RSS.

Мы можем переписать общую стоимость в формуле \ref{eq:4.7.3}, для регрессии лассо как

\begin{equation}
\label{eq:4.9.1}
\begin{gathered}
	Total\text{ }Cost = RSS(w) + \lambda \left\|w\right\|_1
\end{gathered}
\end{equation}

Если $\lambda = 0 \rightarrow \hat{w}^{lasso} = \hat{w}^{Least Squares}$
Если $\lambda = \infty \rightarrow \hat{w}^{lasso} = 0$
Если $\lambda$ между $\rightarrow 0 \leq \hat{w}^{lasso} \leq \hat{w}^{Least Squares}$

Лассо выбирает параметры $\beta$ с минимальным RSS, при условии, что $l_1$ норма параметров $(\left|\beta_1\right| + \left|\beta_2\right| + \cdots + \left|\beta_n\right|) \leq tolerance$.
Ранее мы видели, что оптимальное решение задачи минимизации лассо находится в начале координат, и поэтому мы не можем вычислить градиент.
Поэтому решением является выпуклый алгоритм оптимизации, называемый Координатным спуском. Этот алгоритм пытается минимизировать

\begin{equation}
\label{eq:4.9.2}
\begin{gathered}
	f(w) = f(w_0, w_1, \cdots, w_n)\\
	find \underset{min}{f}(w)
\end{gathered}
\end{equation}


Алгоритм координатного спуска можно описать следующим образом:

\begin{equation}
\label{eq:4.9.3}
\begin{gathered}
	\text{Initialize } \hat{w}\\
	\text{while not converged, pick a coordinate j}\\
	\hat{w}_j \leftarrow \underset{min}{f}(\hat{w}_0. \hat{w}_1, \cdots, w, \hat{w}_{j + 1}, \cdots, \hat{w}_n)
\end{gathered}
\end{equation}

Если мы выберем следующую координату случайным образом, это станет стохастическим координатным спуском.
При координатном спуске нам не нужно выбирать размер шага.


Ниже приведен алгоритм спуска координат для гребневой регрессии, по одной координате за раз.
\begin{equation}
\label{eq:4.9.4}
\begin{gathered}
RSS(w) = \sum_{i = 1}^n (y_i - \sum_{j = 0}^n w_j h_j(x_i))^2\\
\text{Зафиксируем все координаты $w_{-j}$ и возьмем частную производную по $w_j$}\\
\frac{\partial}{\partial w_j} = -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{j = 0}^n w_j h_j(x_i))^2\\
= -2 \sum_{i = 1}^n h_j(x_i)(y_i - \sum_{k \neq j}w_k h_k(x_i) - w_j h_j(x_i))\\
= -2 \sum_{i = 1}^m h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i)) + 2 w_j \sum_{i = 1}^n h_j(x_i)^2\\
\text{Пояснения:}\\
\text{(i) по определению нормированных функций,} \sum_{i = 1}^n h_j(x_i)^2 = 1\\
\text{(ii) мы будем обозначать} (\sum_{i = 1}^n h_j(x_i) (y_i - \sum_{k \neq j} w_k h_k(x_i)) = \rho_j\\
= -2 \rho + 2 w_j\\
\text{приравняв частичные производные к 0, получим}\\
w_j = \rho_j
\end{gathered}
\end{equation}


Ниже приведен псевдокод спуска координат для регрессии лассо, по одной координате за раз.
\begin{equation}
\label{eq:4.9.5}
\begin{gathered}
\text{Initialize } \hat{w}\\
\text{while not converged}\\
\text{for j in } 0,1,2, \cdots,n\\
\text{compute: } \rho_j = \sum_{i = 1}^n h_j(x_i)(y_i ? \hat{y}_i(\hat{w}_j))\\
\text{set: } w_j = \begin{cases}
\rho_j + \frac{\lambda}{2} \text{ if } \rho_j < - \frac{\lambda}{2}\\
0 \text{ if } \rho_j \text{ in } \left[-\frac{\lambda}{2}, \frac{\lambda}{2}\right]\\
\rho_j - \frac{\lambda}{2} \text{ if } \rho_j > \frac{\lambda}{2}
\end{cases}\\
\end{gathered}
\end{equation}


Во \textbf{втором разделе} рассматриваются основные понятия машинного обучения. Приводится описание линейной модели.

Общая постановка задачи машинного обучения. Имеется множество объектов и множество ответов. Между ними существует некоторая неизвестная зависимость. Известно лишь конечное число пар "объект-ответ" составляющее обучающую выборку. По имеющимся данным следует построить алгоритм, который достаточно точно отображает неизвестную зависимость. То есть, способный для любых данных выдать ответ близкий к реальному.

Классы задач машинного обучения:
\begin{itemize}
\item обучение с учителем - восстановление зависимости по известным примерам и ответам.
\item обучение без учителя - известно лишь множество объектов. Множество ответов отсутствует. Требуется, например, найти закономерности
\end{itemize}

Классические задачи машинного обучения:
\begin{itemize}
\item задача классификации. Множество объектов разделено некоторым образом на классы. Имеется обучающая выборка, содержащая пары "объект, класс". Требуется для произвольного объекта определить, к какому классу он мог бы принадлежать
\item задачи кластеризации. Предназначены как для разработки типологии, так и для проверки гипотез на основе исследования данных.
\item задача регрессии. И множество объектов, и множество ответов являются численными данными. Требуется по конечному числу имеющихся точек восстановить исходную зависимость
\end{itemize}

Одна из основных проблем машинного обучения - переобучение. Это явление, при котором для элементов обучающей выборки модель показывает результат, близкий к корректному, а для любого другого работает гораздо хуже.
Это связано с тем, что при обучении модель обнаруживает в выборке случайные закономерности и в итоге "запоминает" все ответы.

Так как обучающая выборка конечна и неполна, а так же достоверно отличить случайные флуктуации от закономерностей невозможно, переобучение будет присутствовать практически всегда. 

Один из факторов, способствующих переобучению, - чрезмерная сложность модели. Зачастую большое количество параметров поощряет подгонку под обучающее множество.

Методы борьбы с переобучением:
\begin{itemize}
\item перекрестная проверка - данные разбиваются на $k$ частей. Обучение модели проходит на $k - 1$, тестирование на одной. 
\item ранняя остановка - как только значение ошибки на тестовых данных начало превышать значение на обучающей выборке, прекращаем обучение
\item увеличение количества обучающих данных - либо искусственно, специальным образом обрабатывая данные, либо путём сбора дополнительной информации 
\item ансамбли моделей - использование нескольких однотипных моделей параллельным или последовательным образом
\item регуляризация
\end{itemize}



Регуляризация - добавление дополнительных условий к задаче с целью предотвратить переобучение.

В данной работе рассматривается применение $L_1$, $L_2$ - регуляризации, про которые было подробно рассказано в 1 главе. Кроме того существует техника регуляризации исключением.

В отличии от вышесказанных техник, регуляризация исключением не может применяться к линейным моделям и вместо изменения функции ошибки, будет меняться сама сеть.

Во время обучения нейросети часть её нейронов, за исключением входных и выходных, случайным образом временно удаляются. В результате чего получается изменённая сеть с меньшим числом нейронов. Далее берётся небольшое число примеров, на них происходит обучение сети. Обновляются соответствующие веса и смещения. Затем восстанавливаются удалённые нейроны и случайным образом выбирается новая группа для удаления.
Одна из возможных реализаций этого алгоритма - вместо удаления фиксированного числа нейронов каждый раз, для каждого нейрона задать вероятность с которым он будет удаляться на новом шаге.

Получаем, когда удаляются разные нейроны, процесс обучения становится похож на обучение сразу нескольких различных нейросетей. И процедура исключения имеет схожий эффект с усреднением по большому числу нейросетей. Разные сети будут переобучаться по-разному и их общий результат может иметь более низкий уровень переобучения.

В \textbf{третьем разделе} находится описание алгоритма применения регуляризации при построении модели, показывающей диагнозы пациентов.

В алгоритме используется SGDClassifier. SGDClassifier - линейный классификатор из библиотеки sklearn. Реализует обучение с помощью стохастического градиентного спуска. Поддерживает $L_1$ и $L_2$ регуляризации.

\begin{enumerate}
\item Импортируем нужные библиотеки
\item Поиск в открытых источниках данных по диагнозам. В работе используются данные с платформы kaggle.
\item Предварительная подготовка данных - чтение всех симптомов и диагнозов. Для каждого диагноза набор симптомов формируется следующим образом: создаётся словарь, где ключом является симптом, а значением - 0 или 1 в зависимости от наличия симптома для соответствующего диагноза
\item Обучение модели без регуляризации и двух моделей с использованием регуляризации с помощью SGDClassifier
\item Сравнение полученных результатов
\end{enumerate}

Обе модели, использующие регуляризацию, показали лучший результат чем модель, которая её не использует.


\begin{center}
    \textbf{Основные результаты}
\end{center}

\begin{enumerate}
\item Изучены основные понятия машинного обучения
\item Определены методы борьбы с переобучением
\item Рассмотрены основные виды регуляризации
\item Обучена модель с использованием и без использования регуляризации
\item В результате сравнения результатов обучения выяснилось, что регуляризация снижает уровень переобучения и точность модели повышается
\end{enumerate}


%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
%\bibliographystyle{ugost2008}
%\bibliographystyle{ugost2008_In} как в каталоге
\bibliographystyle{utf8gost780u} %фамилии курсивом
%\bibliographystyle{utf8gost71u} %фамилии рядом
%\bibliographystyle{ugost2003s}


\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
%\appendix


\end{document}
